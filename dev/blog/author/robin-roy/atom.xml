<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://fury.gl/</id>
  <title>Blog - Posts by Robin Roy</title>
  <updated>2024-06-24T21:15:39.800913+00:00</updated>
  <link href="https://fury.gl/"/>
  <link href="https://fury.gl/blog/author/robin-roy/atom.xml" rel="self"/>
  <generator uri="https://ablog.readthedocs.io/" version="0.11.10">ABlog</generator>
  <entry>
    <id>https://fury.gl/posts/2024/2024-06-06-week-1-robin.html</id>
    <title>Week 1: It officially begins…</title>
    <updated>2024-06-06T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-1-it-officially-begins"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 1.&lt;/p&gt;
&lt;p&gt;My goal for week 1 was to start with &lt;a class="reference external" href="https://www.pinecone.io/learn/retrieval-augmented-generation/"&gt;Retrieval-Augmented Generation (RAG)&lt;/a&gt;, check different databases and host every endpoint. My week1 and week2 are very intertwined because I applied everything I did during week1 on week2. (I’m writing this blog midway through week2)&lt;/p&gt;
&lt;section id="why-phi-3-mini-4k-instruct"&gt;
&lt;h2&gt;why phi-3-mini-4k-instruct?&lt;/h2&gt;
&lt;p&gt;Before I detail everything I’ve done this week, I’ll explain why &lt;a class="reference external" href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"&gt;phi-3 mini 4k&lt;/a&gt; was chosen as the LLM, I forgot to mention this in the last blog. Phi-3 is a small 3.8B 4k context model, it means it can work with 4k tokens(similar to words) at a time. Due to its small size, it runs fast both locally and on Huggingface. Performance wise comparatively with other opensource models, it performs decently well. In the &lt;a class="reference external" href="https://chat.lmsys.org/?leaderboard"&gt;LMSYS LLM leaderboard&lt;/a&gt; phi-3 mini 4k comes with an ELO of 1066 (59th position). But it achieves this as a small model.
I also tried Llama3-8B, it performs better than phi-3 mini with an ELO of 1153 and rank 22. But it is considerably slower for inference. Due to this, I chose phi-3 mini for now.&lt;/p&gt;
&lt;section id="things-i-did-week-1-and-some-week2"&gt;
&lt;h3&gt;Things I did week-1 (and some week2)&lt;/h3&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Choosing the vector database&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I decided to choose &lt;a class="reference external" href="https://www.pinecone.io/"&gt;Pinecone&lt;/a&gt; as the vector DB because it had a very generous free tier. Other options on consideration were &lt;a class="reference external" href="https://github.com/pgvector/pgvector"&gt;pgvector&lt;/a&gt; and &lt;a class="reference external" href="https://www.trychroma.com/"&gt;chromadb&lt;/a&gt;, but they didn’t have a free tier.&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;PR Submissions and Review&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;I also merged a &lt;a class="reference external" href="https://github.com/fury-gl/fury/pull/891"&gt;PR&lt;/a&gt; on FURY which fixes a CI issue. I also spent time doing review of other PRs from my fellow GSoC mates.&lt;/p&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Deciding which embedding model to use&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “3” (ordinal 3)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;A good embedding model is necessary to generate embeddings which we then upsert into the DB. Ollama had embedding model support, but I found the catalogue very small and the models they provided were not powerful enough. Therefore I decided to try using HuggingFace Sentence Transformers.
Sentence Transformers have a very vibrant catalogue of models available of various sizes. I chose &lt;a class="reference external" href="https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5"&gt;gte-large-en-v1.5&lt;/a&gt; from Alibaba-NLP, an 8k context, 434 million parameter model. It only had a modest memory requirement of 1.62 GB.
Performance wise, it ranks 11th on the &lt;a class="reference external" href="https://huggingface.co/spaces/mteb/leaderboard"&gt;MTEB leaderboard&lt;/a&gt;. It is a very interesting model due to its size:performance ratio.&lt;/p&gt;
&lt;ol class="arabic simple" start="4"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hosting the embedding model&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “4” (ordinal 4)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;Hosting this sentence-transformer model was confusing. For some reason, the HF spaces were blocking the Python script from writing on &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;.cache&lt;/span&gt;&lt;/code&gt; folder. Docker container inside spaces runs with user id 1000 (non-root user), therefore I had to give it permission to download and store files.&lt;/p&gt;
&lt;p&gt;I’ve hosted 5 gunicorn workers to serve 5 parallel requests at a time. Since the model is small, this is possible.&lt;/p&gt;
&lt;ol class="arabic simple" start="5"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hosting the database endpoint&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “5” (ordinal 5)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;I wrapped the pinecone DB API into an endpoint so it’ll be easy to query and receive the results.
It is also configured to accept 5 concurrent requests although I could increase it a lot more.&lt;/p&gt;
&lt;p&gt;I upserted docstrings from &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;fury/actor.py&lt;/span&gt;&lt;/code&gt; into the vector DB for testing. So now, whenever you ask a question the model will use some &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;actor.py&lt;/span&gt;&lt;/code&gt; function to give you an answer. For now, it could be used like a semantic function search engine.&lt;/p&gt;
&lt;p&gt;I decided to abstract the DB endpoint to reduce the dependency on one provider. We can swap the providers as required and keep all other features running.&lt;/p&gt;
&lt;ol class="arabic simple" start="6"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hosting Discord Bot&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “6” (ordinal 6)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;So with this, all the endpoints are finally online. The bot has some issues, it is going offline midway for some reason. I’ll have to see why that happens.&lt;/p&gt;
&lt;p&gt;For some reason, Huggingface spaces decided to not start the bot script. Later a community admin from Huggingface told me to use their official bot implementation as a reference. This is why I had to use threading and gradio to get the bot running (migrating to docker can be done, but this is how they did it and I just took that for now).&lt;/p&gt;
&lt;p&gt;Huggingface spaces need a script to satisfy certain criteria to allow them to run, one of them is a non-blocking I/O on the main loop. So I had to move the discord bot to a new thread.&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="connecting-all-of-them-together"&gt;
&lt;h2&gt;Connecting all of them together!&lt;/h2&gt;
&lt;dl class="simple"&gt;
&lt;dt&gt;So now we have 4 hosted services, all hosted on HuggingFace spaces:&lt;/dt&gt;&lt;dd&gt;&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Discord Bot&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;LLM API&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Embeddings API&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Database API&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Now we’ll have to connect them all to get an answer to the user query.&lt;/p&gt;
&lt;p&gt;This is the current architecture, there’s a lot of room for improvement here.&lt;/p&gt;
&lt;blockquote&gt;
&lt;div&gt;&lt;img src="https://github.com/fury-gl/fury-communication-assets/blob/main/gsoc_2024/7-6-2024-demo-architecture-gsoc-robin-week2.png?raw=true"&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;p&gt;The Language Model takes the context and the user query, combines them to form an answer and returns to the user through discord (for now). Maybe moving the core logic from discord bot to a separate node might be good, and connect discord/github/X to that node.
The database takes embeddings and do an Approximate Nearest Neighbor search (a variant of KNN) and returns top-k results (k=3 for now).&lt;/p&gt;
&lt;blockquote&gt;
&lt;div&gt;&lt;iframe src="https://github.com/robinroy03/fury-discord-bot/assets/115863770/48f1136d-18a5-45ee-aa22-0a3f6426d575" width="640" height="390" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h3&gt;What is coming up next week?&lt;/h3&gt;
&lt;p&gt;Answer quality improvements. Also, the discord bot dies randomly, have to fix that also.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h3&gt;Did you get stuck anywhere?&lt;/h3&gt;
&lt;p&gt;Was stuck in hosting models on Huggingface spaces, fixed it later.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/fury-bot-discord/tree/main"&gt;Discord Bot&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/fury-db-endpoint/tree/main"&gt;Database Repo&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/fury-embeddings-endpoint/tree/main"&gt;Embedding Repo&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/fury-bot/tree/main"&gt;LLM Repo&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id1"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “retrieval-augmented generation (rag)”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://www.pinecone.io/learn/retrieval-augmented-generation/"&gt;Retrieval-Augmented Generation (RAG)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id2"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “phi-3 mini 4k”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"&gt;phi-3 mini 4k&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id3"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “lmsys llm leaderboard”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://chat.lmsys.org/?leaderboard"&gt;LMSYS LLM leaderboard&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id4"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “pinecone”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://www.pinecone.io/"&gt;Pinecone&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id5"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “pgvector”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/pgvector/pgvector"&gt;pgvector&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id6"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “chromadb”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://www.trychroma.com/"&gt;chromadb&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id7"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “pr”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/fury-gl/fury/pull/891"&gt;PR&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id8"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “gte-large-en-v1.5”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5"&gt;gte-large-en-v1.5&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id9"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “mteb leaderboard”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/mteb/leaderboard"&gt;MTEB leaderboard&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-06-06-week-1-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 1.</summary>
    <category term="google" label="google"/>
    <published>2024-06-06T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-05-28-week-0-robin.html</id>
    <title>Week 0: Community Bonding!</title>
    <updated>2024-05-29T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-0-community-bonding"&gt;

&lt;p&gt;Hi, I’m Robin and I’m a 2nd year CS undergrad from Vellore Institute of Technology, Chennai. During GSoC ‘24 my work will be to build an LLM chatbot which will help the community by answering their questions.&lt;/p&gt;
&lt;p&gt;Scientific visualization is often complicated and hard for people to get used to - “Although 3D visualization technologies are advancing quickly, their sophistication and focus on non-scientific domains makes it hard for researchers to use
them. In other words, most of the existing 3D visualization and computing APIs are low-level
(close to the hardware) and made for professional specialist developers.” &lt;a class="reference internal" href="../../posts/2024/2024-05-28-week-0-robin.html#fury" id="id1"&gt;&lt;span&gt;[FURY]&lt;/span&gt;&lt;/a&gt;. FURY is our effort to bridge this gap with an easy-to-use API. With LLMs, the goal is to take this one step further and make it even simpler for people to get started. By reducing the barrier to entry, we can bring more people into this domain. Visualization should not be the most time-consuming thing for an engineer/researcher, it is supposed to just happen and help them accelerate faster.&lt;/p&gt;
&lt;section id="my-community-bonding-work"&gt;
&lt;h2&gt;My Community Bonding Work&lt;/h2&gt;
&lt;p&gt;The main goal for me was to try different hosting providers and LLMs, test everything and see how they perform. I had my final exams during this period so I lost around 2 weeks to that. But I did manage to catch up and finish the work.
We wanted to keep hosting cheap (preferably free). I’ll detail all the things I tried and the review for each of them.&lt;/p&gt;
&lt;section id="hosting-work-in-order"&gt;
&lt;h3&gt;Hosting work, in order:&lt;/h3&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ollama on Google Colab&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The way it works is by taking Ollama and running it inside google colab, then providing a reverse proxy using ngrok.
We later connect that reverse proxy to the local ollama instance.&lt;/p&gt;
&lt;p&gt;It works. But Google Colab can run only for a maximum of 12 hours and the runtimes will timeout if idle. Also, it was very hacky.&lt;/p&gt;
&lt;blockquote&gt;
&lt;div&gt;&lt;iframe width="640" height="390" src="https://drive.google.com/file/d/1qNLtXxAMlLQ8xO8jfV0keRtskvcsj-fC/preview" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ollama on Kaggle&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-05-28-week-0-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;Same as above, same issues. Talked with my mentor &lt;a class="reference external" href="https://github.com/m-agour"&gt;Mohamed&lt;/a&gt; and skipped implementation.&lt;/p&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;GGUF (GPT-Generated Unified Format) models with ctransformers on HuggingFace&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-05-28-week-0-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “3” (ordinal 3)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;The way it works is by taking a &lt;a class="reference external" href="https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/"&gt;gguf&lt;/a&gt; model and then inferencing using the ctransformers library from HuggingFace. An endpoint will be exposed using flask/fastapi.&lt;/p&gt;
&lt;p&gt;It had issues like not all models were working, and ctransformers did not support all models. And the models that do work were slow on my machine. Local testing was a nightmare and inference speed on HuggingFace was also very slow.&lt;/p&gt;
&lt;ol class="arabic simple" start="4"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;GGUF models with llama-cpp-python, hosted on HuggingFace&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-05-28-week-0-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “4” (ordinal 4)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;I used langchain wrapper over llama-cpp-python to inference GGUF models. This one was able to handle all GGUF models, and local testing was okayish. When I tried handling concurrent requests, it crashed and gave segmentation fault. I fixed segmentation fault later by increasing gunicorn workers (Gunicorn was the WSGI server I used).
It was still not that good and local testing was annoying me. I cannot iterate fast when it takes a full 2-3 minutes for the output to generate.&lt;/p&gt;
&lt;p&gt;This wrapper on a wrapper on a wrapper was also not fun (langchain wrapper of llama-cpp-python which itself is a wrapper of llama-cpp).&lt;/p&gt;
&lt;p&gt;I later removed langchain and reimplemented everything, but langchain wasn’t the reason for the slow performance so it wasn’t helpful.&lt;/p&gt;
&lt;ol class="arabic simple" start="5"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ollama on HuggingFace!&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-05-28-week-0-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “5” (ordinal 5)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;TLDR: This one worked!&lt;/p&gt;
&lt;blockquote&gt;
&lt;div&gt;&lt;iframe width="640" height="390" src="https://drive.google.com/file/d/17yxdw169uqLlw6WKfi--bWEUQArJk7i2/preview" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;p&gt;Ollama was perfect, it works like a charm on my machine and the ecosystem is also amazing (the people on their discord server are super kind). I knew I had to try ollama on HuggingFace.
I was unable to initially run ollama and provide an endpoint. My dockerfile builds were all failing. Later mentor &lt;a class="reference external" href="https://github.com/skoudoro/"&gt;Serge&lt;/a&gt; told me to use the official Ollama image (till then I was using the Ubuntu base image).&lt;/p&gt;
&lt;p&gt;I managed to get the dockerfile running locally, but still, the HuggingFace build was failing. Then I took help from HuggingFace community. They told me it was HuggingFace blocking some ports, so to try different ports. This is when I came across another ollama server repo, and it was using Ubuntu as the base image. I studied that code and modified my dockerfile. It was adding an env variable to repo settings that I missed. My current dockerfile is just 5 lines and it works well.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="fury-discord-bot"&gt;
&lt;h3&gt;FURY Discord Bot&lt;/h3&gt;
&lt;p&gt;I also made a barebones FURY Discord Bot which was connected to my local ollama instance. My dockerfile was stuck and I wanted to do something tangible, so I did this before the weekly meeting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;div&gt;&lt;iframe src="https://drive.google.com/file/d/17aosa4iyDl90mYfVGPrmILtQdXtS6IEy/preview" width="640" height="480" allow="autoplay"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;p&gt;Currently, I’m finding a vector DB &amp;amp; studying how to effectively use RAG here.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;Yes, I had some issues with the dockerfile. It was resolved.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/fury-bot/tree/main"&gt;HuggingFace repo&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/robinroy03/fury-discord-bot"&gt;Discord Bot&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;div role="list" class="citation-list"&gt;
&lt;div class="citation" id="fury" role="doc-biblioentry"&gt;
&lt;span class="label"&gt;&lt;span class="fn-bracket"&gt;[&lt;/span&gt;&lt;a role="doc-backlink" href="#id1"&gt;FURY&lt;/a&gt;&lt;span class="fn-bracket"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;p&gt;Eleftherios Garyfallidis, Serge Koudoro, Javier Guaje, Marc-Alexandre Côté, Soham Biswas, David Reagan, Nasim Anousheh, Filipi Silva, Geoffrey Fox, and Fury Contributors. “FURY: advanced scientific visualization.” Journal of Open Source Software 6, no. 64 (2021): 3384. &lt;a class="reference external" href="https://doi.org/10.21105/joss.03384"&gt;https://doi.org/10.21105/joss.03384&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-05-28-week-0-robin.html"/>
    <summary>Hi, I’m Robin and I’m a 2nd year CS undergrad from Vellore Institute of Technology, Chennai. During GSoC ‘24 my work will be to build an LLM chatbot which will help the community by answering their questions.</summary>
    <category term="google" label="google"/>
    <published>2024-05-29T00:00:00+00:00</published>
  </entry>
</feed>
