<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://fury.gl/</id>
  <title>Blog - Posts by Robin Roy</title>
  <updated>2024-10-01T04:26:50.515629+00:00</updated>
  <link href="https://fury.gl/"/>
  <link href="https://fury.gl/blog/author/robin-roy/atom.xml" rel="self"/>
  <generator uri="https://ablog.readthedocs.io/" version="0.11.10">ABlog</generator>
  <entry>
    <id>https://fury.gl/posts/2024/2024-08-21-final-report-robin.html</id>
    <title>Google Summer of Code Final Work Product</title>
    <updated>2024-08-21T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;a class="reference external image-reference" href="https://summerofcode.withgoogle.com/programs/2023/projects/ED0203De"&gt;&lt;img alt="https://developers.google.com/open-source/gsoc/resources/downloads/GSoC-logo-horizontal.svg" src="https://developers.google.com/open-source/gsoc/resources/downloads/GSoC-logo-horizontal.svg" style="height: 40px;" /&gt;
&lt;/a&gt;
&lt;a class="reference external image-reference" href="https://summerofcode.withgoogle.com/programs/2023/organizations/python-software-foundation"&gt;&lt;img alt="https://www.python.org/static/img/python-logo&amp;#64;2x.png" src="https://www.python.org/static/img/python-logo&amp;#64;2x.png" style="height: 40px;" /&gt;
&lt;/a&gt;
&lt;a class="reference external image-reference" href="https://fury.gl/latest/index.html"&gt;&lt;img alt="https://python-gsoc.org/logos/fury_logo.png" src="https://python-gsoc.org/logos/fury_logo.png" style="width: 40px;" /&gt;
&lt;/a&gt;
&lt;section id="google-summer-of-code-final-work-product"&gt;

&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Name:&lt;/strong&gt; &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin Roy&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Organization:&lt;/strong&gt; &lt;a class="reference external" href="https://www.python.org/psf-landing/"&gt;Python Software Foundation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sub-Organization:&lt;/strong&gt; &lt;a class="reference external" href="https://fury.gl/latest/index.html"&gt;FURY&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; &lt;a class="reference external" href="https://github.com/fury-gl/fury/wiki/Google-Summer-of-Code-2024-(GSOC2024)#project-2-improving-community-engagement-ai-communication-automation-using-llm"&gt;Improving Community Engagement: AI communication automation using LLM&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;section id="abstract"&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The goal of this project was to implement a &lt;a class="reference external" href="https://developers.google.com/machine-learning/resources/intro-llms"&gt;Large Language Model (LLM)&lt;/a&gt; chatbot that understands the FURY repository. The purpose of the project is to reduce the barrier of entry to scientific visualization. &lt;a class="reference external" href="https://www.pinecone.io/learn/retrieval-augmented-generation/"&gt;Retrieval Augmented Generation (RAG)&lt;/a&gt; was used to get the necessary context for every user query. Multiple variations were explored, including Fine-Tuning models, mixing Fine-Tuning and RAG and RAG alone. Multiple &lt;a class="reference external" href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb"&gt;chunking strategies&lt;/a&gt; were also explored for data collection and storage. The models are served to the user through a Discord Bot and a GitHub App. All the API endpoints are hosted using &lt;a class="reference external" href="https://huggingface.co/robinroy03"&gt;HuggingFace Spaces&lt;/a&gt;. &lt;a class="reference external" href="https://www.pinecone.io/"&gt;Pinecone&lt;/a&gt; was used as the database for storing embeddings. Benchmarking, data collection, and testing were done on &lt;a class="reference external" href="https://github.com/robinroy03/FURY-data-script"&gt;another repository&lt;/a&gt;.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="proposed-objectives"&gt;
&lt;h2&gt;Proposed Objectives&lt;/h2&gt;
&lt;p&gt;The objectives of the GSoC project could be broadly classified as:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;strong&gt;Figuring out hosting.&lt;/strong&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;We were constrained by the need to minimize hosting costs. We managed to complete the whole project with 100% free hosting. Work here included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Experiments with &lt;a class="reference external" href="https://colab.research.google.com/"&gt;Google Colab&lt;/a&gt; notebook hosting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Experiments with &lt;a class="reference external" href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt; notebook hosting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Experiments with &lt;a class="reference external" href="https://huggingface.co/"&gt;HuggingFace&lt;/a&gt; spaces hosting.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;strong&gt;Choosing the technologies to use.&lt;/strong&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;Work here included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Experiments with local &lt;a class="reference external" href="https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/"&gt;GGUF (GPT-Generated Unified Format)&lt;/a&gt; models.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Experiments with different quantizations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Experiments with &lt;a class="reference external" href="https://ollama.com/"&gt;Ollama&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Experiments with &lt;a class="reference external" href="https://github.com/ggerganov/llama.cpp"&gt;LlamaCPP.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Experiments with &lt;a class="reference external" href="https://groq.com/"&gt;Groq&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Experiments with &lt;a class="reference external" href="gemini.google.com"&gt;Google Gemini&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;strong&gt;Work on the backend architecture.&lt;/strong&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;Backend architecture was heavily influenced by HuggingFace and its limitations. Work here included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Choosing the API architecture.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Integrating different models.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Improving concurrent requests support.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Improving the UX of the endpoints.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;strong&gt;Work on improving model accuracy.&lt;/strong&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;This was a recurring work and kept happening on most weeks. It included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Model Benchmarking&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data Collection&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Experiments on Retrieval Augmented Generation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Experiments on Fine-Tuning.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Experiments on Chunking.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Experiments on Retrieval quantity.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;strong&gt;Discord Bot integration.&lt;/strong&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;The work included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Building the Discord Bot.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Improving the UX of the bot.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Improving the performance of the bot.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;strong&gt;GitHub App integration.&lt;/strong&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;The work included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Building the GitHub App integration.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Improving the UX of the integration.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="objectives-completed"&gt;
&lt;h2&gt;Objectives Completed&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Figuring out hosting.&lt;/strong&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;As mentioned, we had a constraint on the cost. We explored different options for free hosting. This took us to explore interesting directions like Google Colab and Kaggle Notebooks. In the end, HuggingFace was decided to be the best place. Everything is containerized and currently hosted on HuggingFace.&lt;/p&gt;
&lt;p&gt;This also meant that all the upcoming design/architectural choices would have to be based on HuggingFace. This will cause some challenges on the Discord bot hosting but overall HuggingFace was a solid choice.&lt;/p&gt;
&lt;p&gt;A very detailed blog on hosting is available &lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-05-28-week-0-robin.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The plan is to move all the HuggingFace repositories from my account to FURY’s account. But here, I’ll link to all my repositories which are currently active as I’m writing this report.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/fury-embeddings-endpoint/tree/main"&gt;Embeddings Endpoint&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;This endpoint converts natural language to embeddings. The model is loaded using HuggingFace SentenceTransformer.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/ollama-server-backend/tree/main"&gt;Ollama Endpoint&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;This endpoint could be used to communicate with the Ollama models. The perk of using this is it is more convenient and generally faster. A separate repository was required because a single free HuggingFace Space cannot allocate more than 16 GB RAM and 2vCPUs. Token generation speed will be hit if it’s not a separate repository.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/fury-db-endpoint/tree/main"&gt;Database Endpoint&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;This endpoint was used to get the K-Nearest (or Approximate) embeddings based on cosine similarity. The parameter K could be passed to adjust it. We used Pinecone as the database.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/Fury-Discord-Bot/tree/main"&gt;FURY Discord Bot&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;The repository for the Discord bot. It was required to use threading here which is a side-effect of HuggingFace Spaces. HuggingFace server only activates once there is an active live endpoint. Discord did not need an endpoint, but we had to make one to get the server activated. The Discord bot ran on a separate thread while a server ran on the main thread.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/fury-bot/tree/main"&gt;FURY external cloud endpoints&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;This repository orchestrated external APIs from 3rd party providers like Groq and Gemini. We made it a separate repo to abstract the logic and simplify calling different endpoints as required. You can hot-swap multiple LLM models by changing the REST API parameters.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/github-bot/tree/main"&gt;GitHub App&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;Repository for the GitHub application. Receives webhooks from GitHub and acts upon them using GraphQL queries.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/fury-engine/tree/main"&gt;FURY Engine&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;This is the main endpoint both Discord and GitHub frontend applications hit. It orchestrates all the other endpoints. The architecture of how it works is detailed later below.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;a class="reference external" href="https://github.com/robinroy03/FURY-data-script"&gt;FURY Data Parsing/Benchmarking/Testing Repo (GitHub)&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;This is a GitHub repository and contains all the parsing, benchmarking and testing scripts.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Choosing the technologies to use&lt;/strong&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;Choosing the technology depended largely on HuggingFace hardware support. We experimented with inferencing LlamaCPP directly, inferencing Ollama, tested different quantizations and so on. Phi-3-mini-4k-instruct was chosen initially as the LLM. We rolled with it using Ollama for a few weeks. But as luck has it, I ended up discovering Groq is a cloud provider that provides free LLM endpoints. We used Groq from then on, and later also integrated Gemini since they also have a free tier.&lt;/p&gt;
&lt;p&gt;You can hot-swap between a local model, a Groq model, a Gemini normal model or a Gemini Fine-Tuned model as you wish using the FURY Engine endpoint. it’ll all integrate cleanly with the Pinecone database outputs and give a standard API response.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Work on the backend architecture&lt;/strong&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;This is the present backend architecture.&lt;/p&gt;
&lt;img alt="Present backend architecture" src="https://fury.gl/_images/gsoc_llm_robin_week5.jpg" /&gt;
&lt;p&gt;You’re only hitting the FURY Engine endpoint, the remaining are all abstracted away. You can tell the engine you need to use Gemini and it’ll do that for you. This is also expandable, if you have a new provider, you can add a new endpoint and connect it to FURY Engine.&lt;/p&gt;
&lt;p&gt;The data to the REST endpoint will look like this&lt;/p&gt;
&lt;div class="highlight-json notranslate"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="nt"&gt;&amp;quot;query&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Render a cube in fury&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;&amp;quot;llm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;llama3-70b-8192&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;&amp;quot;knn&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;&amp;quot;stream&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;false&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Every output response will look like this&lt;/p&gt;
&lt;div class="highlight-json notranslate"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="nt"&gt;&amp;quot;response&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Yes, this is how it would be done python import fury....&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;&amp;quot;references&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;1, 2, 3&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;So if you do&lt;/p&gt;
&lt;p&gt;&lt;cite&gt;curl -X POST https://robinroy03-fury-engine.hf.space/api/groq/generate -H “Content-Type: application/json” -d ‘{“query”: “How do I create a sphere in FURY?”, “llm”: “llama3-70b-8192”, “knn”: “3”, “stream”: false}’&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;You’ll get a response from &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;llama3-70b-8192&lt;/span&gt;&lt;/code&gt; using &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Groq&lt;/span&gt;&lt;/code&gt;. If you do &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;https://robinroy03-fury-engine.hf.space/api/google/generate&lt;/span&gt;&lt;/code&gt; you can call any Google Gemini models like &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;gemini-1.5-pro&lt;/span&gt;&lt;/code&gt; or &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;gemini-1.5-flash&lt;/span&gt;&lt;/code&gt;. Same for &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Ollama&lt;/span&gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A detailed blog on architecture is available &lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-07-01-week-5-robin.html"&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Work on improving model accuracy&lt;/strong&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;The initial version had major issues of hallucination and was unable to retrieve relevant context. We fix them by collecting more data, improving RAG, setting up a benchmark and so on.&lt;/p&gt;
&lt;p&gt;The Initial version used a naive parser to parse code, later my mentors told me to use an AST parser. I chunked the entire repo using this and it performed relatively better. For model benchmarking, we had 2 tests, one QnA testing and one code testing. If the code compiles, the model gets one point.&lt;/p&gt;
&lt;p&gt;All the benchmarking, data parsing, and database upsertion scripts are &lt;a class="reference external" href="https://github.com/robinroy03/FURY-data-script"&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We used an image model called &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;moondream2&lt;/span&gt;&lt;/code&gt; to validate the output generated by the model. Since FURY is a graphics library, we need to judge the image to see whether it is correct or not.&lt;/p&gt;
&lt;p&gt;Fine-tuning was done on Google AI Studio. We Fine-Tuned using question/answer pairs from Discord and GitHub discussions. We later tried mixing RAG + Fine-Tuning. A detailed blog on Fine-Tuning is available &lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-07-27-week-8-robin.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A detailed blog on benchmarking is available &lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-07-01-week-5-robin.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A detailed blog on chunking is available &lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-06-16-week-2-robin.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Discord Bot integration&lt;/strong&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;This included building the Discord bot and connecting it with the backend API. As mentioned above, threading was used to get the bot running on the server. But this won’t affect any other part of the bot and it’ll work as usual.&lt;/p&gt;
&lt;p&gt;This is what the discord integration looks like:&lt;/p&gt;
&lt;img alt="Present Discord Bot UI." src="https://fury.gl/_images/gsoc_robin_discord.jpg" /&gt;
&lt;p&gt;The code runs! This is the output of the code:&lt;/p&gt;
&lt;img alt="Output of the code." src="https://fury.gl/_images/gsoc_robin_discord_demo.jpg" /&gt;
&lt;p&gt;Work was also done on improving the UX of the bot. There are 👍 and 👎 options available for the user to rate the answer. We’ll use those signals to improve the bot further. There are reference links at the bottom that lead to the exact places where the answers are sourced from. You can technically also use the Discord bot as a search engine if you want to.&lt;/p&gt;
&lt;p&gt;Initially, the bot had a sync over async problem. It was later fixed. Now multiple people can converse with the bot simultaneously.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;GitHub App integration&lt;/strong&gt;&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;This included building the GitHub app and figuring out how to setup the UX for it. GitHub used GraphQL, but we didn’t use a separate GraphQL library for this. We used a custom setup to query GraphQL endpoints. For us who only work with 1 or 2 commands, it works well. The code is &lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/github-bot/tree/main"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;GitHub App UI looks like this:&lt;/p&gt;
&lt;img alt="Present GitHub App UI." src="https://fury.gl/_images/robin_gsoc_github_ui.jpg" /&gt;
&lt;p&gt;It is similar to Discord because the results come from the same backend. Refer to the backend architecture above for reference.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="other-objectives"&gt;
&lt;h2&gt;Other Objectives&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;strong&gt;Improving the LLM output&lt;/strong&gt; (ongoing)&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;This will continue till I’m satisfied. It’s a never ending journey :) Much of this GSoC was setting up things and getting it all to work as one piece. There are tons of new ideas coming up every day to increase LLM accuracy. I’ll explore them and try interesting ones.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;strong&gt;Tests for all endpoints&lt;/strong&gt; (ongoing)&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;It’s important to have tests for all endpoints. Testing includes the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Check the endpoints with valid data to see the response. Validate the JSON format.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Check the endpoints with incorrect schema and record the response.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Test by adjusting parameters like KNN.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="simple"&gt;
&lt;dt&gt;&lt;strong&gt;X Bot&lt;/strong&gt; (Optional Goal, deferred for now)&lt;/dt&gt;&lt;dd&gt;&lt;p&gt;I had a talk about this with my mentors. This can be done by plugging the LLM backend into an X bot frontend, but they suggested spending my time improving model accuracy rather than simply adding another frontend for the LLM application.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="other-open-source-tasks"&gt;
&lt;h2&gt;Other Open Source tasks&lt;/h2&gt;
&lt;p&gt;GSoC isn’t all about what I do with my project. It exists along with the 3 other cool projects my peers - &lt;a class="reference external" href="https://github.com/WassCodeur"&gt;Wachiou&lt;/a&gt;, &lt;a class="reference external" href="https://github.com/itellaetxe"&gt;Iñigo&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/deka27"&gt;Kaustav&lt;/a&gt;   did. I learnt a lot through them reviewing my PRs and me reviewing their PRs. I attended all the weekly meetings of Wachiou to learn about his progress and to learn new stuff. He attended all my meetings too, which was awesome :)&lt;/p&gt;
&lt;dl class="simple"&gt;
&lt;dt&gt;Contributions to FURY apart from the ones directly part of GSoC:&lt;/dt&gt;&lt;dd&gt;&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="github reference external" href="https://github.com/fury-gl/fury/pull/862"&gt;fury-gl/fury#862&lt;/a&gt; - Rendering videos on a cube&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="github reference external" href="https://github.com/fury-gl/fury/pull/861"&gt;fury-gl/fury#861&lt;/a&gt; - docstring improvements&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="github reference external" href="https://github.com/fury-gl/fury/pull/891"&gt;fury-gl/fury#891&lt;/a&gt; - Codespell fix&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="github reference external" href="https://github.com/fury-gl/fury/pull/893"&gt;fury-gl/fury#893&lt;/a&gt; - .gitignore modification&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="github reference external" href="https://github.com/fury-gl/fury/issues/924"&gt;fury-gl/fury#924&lt;/a&gt; - Raised issue&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;dt&gt;Contributions to other repositories during this time, due to GSoC work:&lt;/dt&gt;&lt;dd&gt;&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="github reference external" href="https://github.com/langchain-ai/langchain/issues/23515"&gt;langchain-ai/langchain#23515&lt;/a&gt; - Langchain issue raised&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="github reference external" href="https://github.com/github/docs/issues/34258"&gt;github/docs#34258&lt;/a&gt; - GitHub issue raised&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="github reference external" href="https://github.com/github/docs/pull/34259"&gt;github/docs#34259&lt;/a&gt; - PR for the raised GitHub issue&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="github reference external" href="https://github.com/orgs/community/discussions/136436"&gt;orgs/community#136436&lt;/a&gt; - GitHub feature request&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/section&gt;
&lt;section id="acknowledgement"&gt;
&lt;h2&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;I am very thankful to my mentors &lt;a class="reference external" href="https://github.com/skoudoro"&gt;Serge Koudoro&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/m-agour"&gt;Mohamed Abouagour&lt;/a&gt;. They were awesome and provided me with a comfortable environment to work in. Also got to thank &lt;a class="reference external" href="https://www.linkedin.com/in/3v3ryone"&gt;Beleswar Prasad Padhi&lt;/a&gt; who gave me a very good introduction to opensource. The good thing about open source is I can still work on this (and other FURY projects) till I’m satisfied. I’m excited to continue contributing to the open source community.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="timeline"&gt;
&lt;h2&gt;Timeline&lt;/h2&gt;
&lt;table class="table" id="id1"&gt;
&lt;caption&gt;&lt;span class="caption-text"&gt;GSoC 2024 Weekly Reports&lt;/span&gt;&lt;/caption&gt;
&lt;colgroup&gt;
&lt;col style="width: 13.0%" /&gt;
&lt;col style="width: 43.5%" /&gt;
&lt;col style="width: 43.5%" /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class="row-odd"&gt;&lt;th class="head"&gt;&lt;p&gt;Week&lt;/p&gt;&lt;/th&gt;
&lt;th class="head"&gt;&lt;p&gt;Description&lt;/p&gt;&lt;/th&gt;
&lt;th class="head"&gt;&lt;p&gt;Blog Post Link&lt;/p&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="row-even"&gt;&lt;td&gt;&lt;p&gt;Week 0&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;Community Bonding!&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-05-28-week-0-robin.html"&gt;Blog 0&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="row-odd"&gt;&lt;td&gt;&lt;p&gt;Week 1&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;It officially begins…&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-06-06-week-1-robin.html"&gt;Blog 1&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="row-even"&gt;&lt;td&gt;&lt;p&gt;Week 2&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;The first iteration!&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-06-16-week-2-robin.html"&gt;Blog 2&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="row-odd"&gt;&lt;td&gt;&lt;p&gt;Week 3&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;Data Data Data!&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-06-16-week-3-robin.html"&gt;Blog 3&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="row-even"&gt;&lt;td&gt;&lt;p&gt;Week 4&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;Pipeline Improvements and Taking The Bot Public!&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-07-01-week-4-robin.html"&gt;Blog 4&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="row-odd"&gt;&lt;td&gt;&lt;p&gt;Week 5&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;LLM Benchmarking &amp;amp; Architecture Modifications&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-07-01-week-5-robin.html"&gt;Blog 5&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="row-even"&gt;&lt;td&gt;&lt;p&gt;Week 6&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;UI Improvements and RAG performance evaluation&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-07-27-week-6-robin.html"&gt;Blog 6&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="row-odd"&gt;&lt;td&gt;&lt;p&gt;Week 7&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;Surviving final examinations&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-07-27-week-7-robin.html"&gt;Blog 7&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="row-even"&gt;&lt;td&gt;&lt;p&gt;Week 8&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;Gemini Finetuning&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-07-27-week-8-robin.html"&gt;Blog 8&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="row-odd"&gt;&lt;td&gt;&lt;p&gt;Week 9&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;Hosting FineTuned Models&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-08-15-week-9-robin.html"&gt;Blog 9&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="row-even"&gt;&lt;td&gt;&lt;p&gt;Week 10&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;Learning GraphQL&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-08-16-week-10-robin.html"&gt;Blog 10&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="row-odd"&gt;&lt;td&gt;&lt;p&gt;Week 11&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;Getting the App Live&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-08-17-week-11-robin.html"&gt;Blog 11&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="row-even"&gt;&lt;td&gt;&lt;p&gt;Week 12&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;Wrapping things up&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-08-20-week-12-robin.html"&gt;Blog 12&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-08-21-final-report-robin.html"/>
    <summary>Name: Robin Roy</summary>
    <category term="google" label="google"/>
    <published>2024-08-21T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-08-20-week-12-robin.html</id>
    <title>Week 12: Wrapping things up</title>
    <updated>2024-08-20T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-12-wrapping-things-up"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 12.&lt;/p&gt;
&lt;p&gt;As the final official week, I spent my time wrapping things up and also improving the UX of the GitHub Application.&lt;/p&gt;
&lt;section id="things-i-did-in-week-12"&gt;
&lt;h2&gt;Things I did in Week 12&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Improving GitHub App UX&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Previously the bot responded to every discussion post. It was not a good approach and we tried stuff like &amp;#64;mentions. The problem is GitHub does not support bot mentions natively. Actually &lt;a class="reference external" href="https://github.com/skoudoro/"&gt;Serge&lt;/a&gt; had a better approach that is using Discussion Templates. I integrated that. Right now, you have a checkbox that you can tick to get the LLM answer as the first response.&lt;/p&gt;
&lt;p&gt;The new UI looks like this:&lt;/p&gt;
&lt;img alt="Present GitHub Discussions Template" src="https://fury.gl/_images/robin_gsoc_FURY_DISCUSSIONS_TEMPLATE.jpg" /&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-08-20-week-12-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;Some of the API endpoints had no documentation, the documentation work is still ongoing. But I worked on adding basic info like how to test locally and stuff. It was added directly to the README.md files. I’ll also make a separate GitHub Gists where I’ll detail all the components and how they integrate with each other.&lt;/p&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;API testing&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-08-20-week-12-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “3” (ordinal 3)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;I plan to have testing for every endpoint. Testing includes the following:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Check the endpoints with valid data to see the response. Validate the JSON format.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Check the endpoints with incorrect schema and record the response.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Test by adjusting parameters like KNN.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Testing will be a separate file, it’ll be production testing. We’ll hit the live endpoints directly.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;p&gt;I’m working on the final report. Also, I’m working on finishing testing, documentation and updating the LLM response. The plan is to use a Re-Ranker to rerank the KNN references and filter ones not in context.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;No, I did not get stuck. I was having some health issues this week so was unable to make a lot of progress. But the general plan is prepared, and now I’ll have to compile everything.&lt;/p&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-08-20-week-12-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 12.</summary>
    <category term="google" label="google"/>
    <published>2024-08-20T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-08-17-week-11-robin.html</id>
    <title>Week 11: Getting the App Live</title>
    <updated>2024-08-17T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-11-getting-the-app-live"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 11.&lt;/p&gt;
&lt;p&gt;This week I worked on Getting the GitHub App live.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Getting the App Live&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Last week I prototyped and got to know the language and the API. But I can’t use my account as an automated bot account. So it was required to make a GitHub App. The architecture for it is as follows:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Make a GitHub App to listen to Discussion posts&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Send a webhook to &lt;a class="reference external" href="https://robinroy03-github-bot.hf.space/github"&gt;https://robinroy03-github-bot.hf.space/github&lt;/a&gt; whenever any change happens.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Respond to the webhook as required.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I was told by &lt;a class="reference external" href="https://github.com/skoudoro"&gt;Serge&lt;/a&gt; to try and fit the endpoint inside the Discord Bot script. I tried but it was weird so I left it. The Discord Bot is set up using threading which is a hack (although it is how every discord bot is set up in HuggingFace). Placing it inside any other repository won’t be good so I ended up making another new repository.&lt;/p&gt;
&lt;p&gt;I faced an issue while trying to get the app live. I had another documentation rabbit hole situation. So what ended up happening was I was unable to authenticate myself with the GitHub app to send commands. To command an app you have to authenticate as a &lt;cite&gt;GitHub App Installation&lt;/cite&gt;. To authenticate as an App installation, you need 3 key things:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Installation ID&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;App ID&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Private Key of the App&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You use the &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;App&lt;/span&gt; &lt;span class="pre"&gt;ID&lt;/span&gt;&lt;/code&gt; and &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Private&lt;/span&gt; &lt;span class="pre"&gt;Key&lt;/span&gt;&lt;/code&gt; to make a &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;JWT&lt;/span&gt;&lt;/code&gt;. You use the &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;JWT&lt;/span&gt;&lt;/code&gt; with &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Installation&lt;/span&gt; &lt;span class="pre"&gt;ID&lt;/span&gt;&lt;/code&gt; to make an &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Installation&lt;/span&gt; &lt;span class="pre"&gt;Access&lt;/span&gt; &lt;span class="pre"&gt;Token&lt;/span&gt;&lt;/code&gt;. You’ll now use this &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Installation&lt;/span&gt; &lt;span class="pre"&gt;Access&lt;/span&gt; &lt;span class="pre"&gt;Token&lt;/span&gt;&lt;/code&gt; to authorize you and then send commands to the GitHub App. The &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Access&lt;/span&gt; &lt;span class="pre"&gt;Token&lt;/span&gt;&lt;/code&gt; will expire after 1 hour, so we’ll have to regenerate it.&lt;/p&gt;
&lt;p&gt;The problem was that the documentation didn’t mention how to generate &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Installation&lt;/span&gt; &lt;span class="pre"&gt;Access&lt;/span&gt; &lt;span class="pre"&gt;Token&lt;/span&gt;&lt;/code&gt; and it kept confusing everyone with &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Installation&lt;/span&gt; &lt;span class="pre"&gt;ID&lt;/span&gt;&lt;/code&gt;. Even the names were misleading, since it isn’t an &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Installation&lt;/span&gt; &lt;span class="pre"&gt;Token&lt;/span&gt;&lt;/code&gt; in the real sense cause it is already installed. I ended up fixing it by landing at &lt;a class="reference external" href="https://stackoverflow.com/questions/77325437/how-do-i-get-an-github-app-installation-token-to-authenticate-cloning-a-reposito"&gt;this StackOverflow Post&lt;/a&gt; which took me to this &lt;a class="reference external" href="https://github.com/orgs/community/discussions/48186"&gt;Discussions Post&lt;/a&gt;. I think the majority uses &lt;cite&gt;Octokit.js SDK&lt;/cite&gt; to generate Access Tokens and regenerate JWTs. Sadly Python has no library so we had to go all manual.&lt;/p&gt;
&lt;p&gt;So I ended up sending a PR to GitHub Docs :)&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Issue: &lt;a class="github reference external" href="https://github.com/github/docs/issues/34258"&gt;github/docs#34258&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;PR: &lt;a class="github reference external" href="https://github.com/github/docs/pull/34259"&gt;github/docs#34259&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can test the GitHub App today! Checkout &lt;a class="github reference external" href="https://github.com/robinroy03/FURY-data-script/discussions"&gt;robinroy03/FURY-data-script#discussions&lt;/a&gt;&lt;/p&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Week 12 :) I’ll be finalizing stuff.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make the GitHub App respond to mentions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;Was stuck with the documentation but got it fixed.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://stackoverflow.com/questions/77325437/how-do-i-get-an-github-app-installation-token-to-authenticate-cloning-a-reposito"&gt;StackOverflow Post&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-08-17-week-11-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id1"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “discussions post”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/orgs/community/discussions/48186"&gt;Discussions Post&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="github reference external" href="https://github.com/github/docs/issues/34258"&gt;github/docs#34258&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="github reference external" href="https://github.com/github/docs/pull/34259"&gt;github/docs#34259&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-08-17-week-11-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 11.</summary>
    <category term="google" label="google"/>
    <published>2024-08-17T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-08-16-week-10-robin.html</id>
    <title>Week 10: Learning GraphQL</title>
    <updated>2024-08-16T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-10-learning-graphql"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 10.&lt;/p&gt;
&lt;p&gt;This week I worked on the GitHub GraphQL implementation. I tested out things and was learning GraphQL properly since I have never used it before.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Learning and testing GraphQL&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I spent time learning and implementing prototypes of the GitHub app. Initially, I tested using &lt;a class="reference external" href="https://docs.github.com/en/graphql/overview/explorer"&gt;GitHub Explorer&lt;/a&gt; to control my account. I initially spent some time searching for other GitHub libraries but later gave up and made my custom scripts. There are no Python GraphQL libraries available, and the ones available do not integrate with Discussion tabs.&lt;/p&gt;
&lt;p&gt;I used Explorer to mainly focus only on the query language, and not on implementation. The plan was to use a GitHub app to send webhooks to the HuggingFace server, which will respond to it.&lt;/p&gt;
&lt;p&gt;We use &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;query&lt;/span&gt;&lt;/code&gt; to fetch discussions and &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;mutation&lt;/span&gt;&lt;/code&gt; to send a reply. They are GraphQL operations.&lt;/p&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Working GitHub App&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;No, I did not get stuck anywhere. I was learning new things and experimenting with stuff.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-08-16-week-10-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id1"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “github explorer”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://docs.github.com/en/graphql/overview/explorer"&gt;GitHub Explorer&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-08-16-week-10-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 10.</summary>
    <category term="google" label="google"/>
    <published>2024-08-16T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-08-15-week-9-robin.html</id>
    <title>Week 9: Hosting FineTuned Models</title>
    <updated>2024-08-15T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-9-hosting-finetuned-models"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 9.&lt;/p&gt;
&lt;p&gt;This week I worked on hosting the Finetuned model as an API and started work on GitHub GraphQL.&lt;/p&gt;
&lt;section id="things-i-did-in-week-9"&gt;
&lt;h2&gt;Things I did in Week 9&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hosting the fine-tuned API&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Last week we fine-tuned the Gemini model, but it didn’t have an endpoint which we could use to connect with Discord/other frontend applications. I thought it would be a simple task until I realized it wasn’t. Some features are still in beta phase, like this one :)&lt;/p&gt;
&lt;p&gt;Fine-tuned models need more permissions to be used under an API, cause it is your data (as per Google policy). Google Gemini API provides only 1 way to achieve this right now, and that is by using a short-lived token. Short-lived tokens can’t be used on a server cause we’ll have to rotate it, and to rotate them I’ll need to sign in to my Google account every time, and I can’t program it.&lt;/p&gt;
&lt;p&gt;The way we generally solve this is by using a token with no expiry - but the Gemini API does not support that. I tried making service accounts to bypass expiry but it was all failing. The documentation does not mention anywhere how to fix this issue either.&lt;/p&gt;
&lt;p&gt;After a lot of googling, I ended up checking the &lt;a class="reference external" href="https://github.com/google-gemini/cookbook/"&gt;Google Gemini Cookbook repo&lt;/a&gt;, here we have a notebook which talks about this problem! I was so happy seeing &lt;a class="reference external" href="https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication_with_OAuth.ipynb"&gt;this Authentication_with_OAuth.ipynb file&lt;/a&gt;. The solution is to essentially add permission to the fine-tuned model through a REST call. There is no UI/SDK way to do this. You’ll have to trigger a certain REST endpoint to update the permissions to “EVERYONE” so anyone can access the fine-tuned model. For FURY it’s fine since FURY does not contain any sensitive information.&lt;/p&gt;
&lt;dl class="simple"&gt;
&lt;dt&gt;So right now our workflow is as follows:&lt;/dt&gt;&lt;dd&gt;&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Fine-tune a model on Google AI Studio.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Update model permissions using a separate script.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Call through the FURY-Engine API as usual.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;GraphQL work&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-08-15-week-9-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;The next thing I did was start working on GitHub integration. The Discord Bot is hosted and stable, now it was time to do the same with GitHub. For GitHub, the aim is to use the LLM to give a first response to discussions posts. GitHub uses GraphQL instead of REST APIs.&lt;/p&gt;
&lt;p&gt;If you do not know GraphQL you can learn about it in detail from &lt;a class="reference external" href="https://www.youtube.com/playlist?list=PL4cUxeGkcC9gUxtblNUahcsg0WLxmrK_y"&gt;this YouTube playlist&lt;/a&gt; and later from the &lt;a class="reference external" href="https://graphql.org/"&gt;official docs&lt;/a&gt;. But I’ll give you a quick explanation anyway since I think the playlist and docs miss this part:&lt;/p&gt;
&lt;p&gt;GraphQL is essentially HTTP POST/GET calls. We’ll avoid all the jargon here and talk from first principles. REST API philosophy is to provide multiple endpoints &lt;cite&gt;/google&lt;/cite&gt;, &lt;cite&gt;/groq&lt;/cite&gt;, etc (these are FURY-engine endpoints). They do different things. Now these are just styles, remember that. At the end of the day you’re still sending network packets to the server, these just dictate which URL you send it to and what data it contains.&lt;/p&gt;
&lt;p&gt;GraphQL is different in the sense it does not have multiple endpoints. There’s only one endpoint (example: &lt;a class="reference external" href="https://api.github.com/graphql"&gt;https://api.github.com/graphql&lt;/a&gt; for GitHub). We send all our requests to this endpoint and then the server uses it to do an action and return results. So you may ask “Why” do we need to follow the GraphQL syntax, why not just modify REST API to follow our custom style? You can do that. GraphQL is just a style of doing things that smart people at Meta decided to standardize.&lt;/p&gt;
&lt;p&gt;The reason people use GraphQL is because it reduces the number of queries required. You can read the docs to see example GraphQL queries, it is compact and you can easily get a lot of information with one single call. Different people have different opinions about how to make and consume APIs. But fundamentally it’s just another layer of abstraction.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Work on GitHub App.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;I was stuck with the Gemini API part but it was fixed. It was also a learning experience to not trust documentation always :)&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-08-15-week-9-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id1"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “google gemini cookbook repo”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/google-gemini/cookbook/"&gt;Google Gemini Cookbook repo&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication_with_OAuth.ipynb"&gt;Authentication_with_OAuth.ipynb file&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://www.youtube.com/playlist?list=PL4cUxeGkcC9gUxtblNUahcsg0WLxmrK_y"&gt;GraphQL YouTube playlist&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://graphql.org/"&gt;GraphQL official docs&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-08-15-week-9-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 9.</summary>
    <category term="google" label="google"/>
    <published>2024-08-15T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-07-27-week-8-robin.html</id>
    <title>Week 8: Gemini Finetuning</title>
    <updated>2024-07-27T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-8-gemini-finetuning"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 8.&lt;/p&gt;
&lt;p&gt;This week I worked on finalizing the Discord chat QnA data collection and using it to Fine-Tune the Gemini-1.0-Pro model.&lt;/p&gt;
&lt;section id="things-i-did-in-week-8"&gt;
&lt;h2&gt;Things I did in Week 8&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Discord Data Collection&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I finished collecting data from all the channels in the Discord server, cross-verifying to check whether they still work. I also added some questions which were on the FURY bot testing server. These QnA pairs were later converted to a CSV with input/output pairs and fed to Gemini for finetuning.&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Gemini Finetuning&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-27-week-8-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;Finetuning is essentially training the model on the input/output. RAG is giving context and asking the model to form an answer using that. Finetuning updates the model weights as per the input/output. Gemini uses &lt;a class="reference external" href="https://huggingface.co/blog/peft"&gt;Parameter-Efficient Fine-Tuning&lt;/a&gt; in AI Studio as per some reports. It makes sense because the tuning only takes minutes and PEFT is a good strategy to prevent issues like &lt;a class="reference external" href="https://arxiv.org/abs/1312.6211"&gt;catastrophic forgetting&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finetuning and RAG are complementary to each other. The difference between them can be summarized as follows:&lt;/p&gt;
&lt;p&gt;RAG is like giving an LLM with no prior knowledge about FURY access to some important functions/classes as per the user prompt. It’ll use this given context and its knowledge of graphics libraries (knowledge from pretraining) to form an answer.&lt;/p&gt;
&lt;p&gt;Finetuning is used to make the model follow a certain style or behaviour. It is a form of mimicking the input-output. This will help in increasing the model’s performance. An interesting thing is I had to train the model 1) with RAG and 2) without RAG.&lt;/p&gt;
&lt;p&gt;For finetuning, the input must be in the format the LLM will get the answer from the user. When you ask a question to the FURY bot, the bot does not get your question directly. We are processing it to add additional information. Therefore I had to process all the collected data with RAG.&lt;/p&gt;
&lt;p&gt;This is an interesting direction, and I have a lot of cool things to try out here. I’ll spend the next few weeks trying different ideas.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Finetuning strategies.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hosting the model on API.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;No, I did not get stuck anywhere.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-27-week-8-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id1"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “parameter-efficient fine-tuning”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/blog/peft"&gt;Parameter-Efficient Fine-Tuning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-27-week-8-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id2"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “catastrophic forgetting”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://arxiv.org/abs/1312.6211"&gt;catastrophic forgetting&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-07-27-week-8-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 8.</summary>
    <category term="google" label="google"/>
    <published>2024-07-27T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-07-27-week-7-robin.html</id>
    <title>Week 7: Surviving final examinations</title>
    <updated>2024-07-27T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-7-surviving-final-examinations"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 7.&lt;/p&gt;
&lt;p&gt;I majorly took this week off due to my semester final examinations :) They were fun. Major topics were x86, ARM and 8051. I had not written a lot of assembly apart from school work. I took the week to experiment with some assembly. The course was more into hardware architecture than programming. I’ve now enough knowledge to read a given piece of ASM code with a wiki to look up mnemonics (and Gemini/Claude to help). I’m not fast in writing ASM (yet), one day I’ll find a project to dive into, or maybe some reverse engineering and CTFs. GPU instruction sets are also something interesting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Discord data collection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I collected some Q&amp;amp;A questions from the FURY discord server. I did it manually because the volume wasn’t high, and I wanted it to be correct. Had to cross-check with GitHub also to check whether the answer/code mentioned still stands. The format I used was [User question, Answer]. If the answer/question is spread across multiple conversations, I’ll adjust it to this format.&lt;/p&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Gemini Finetuning&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Collect more Discord data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;Not really apart from some silly ASM bugs.&lt;/p&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-07-27-week-7-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 7.</summary>
    <category term="google" label="google"/>
    <published>2024-07-27T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-07-27-week-6-robin.html</id>
    <title>Week 6: UI Improvements and RAG performance evaluation</title>
    <updated>2024-07-27T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-6-ui-improvements-and-rag-performance-evaluation"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 6.&lt;/p&gt;
&lt;p&gt;This week, I worked on some UI improvements and studied and evaluated the RAG performance.&lt;/p&gt;
&lt;section id="things-i-did-in-week-6"&gt;
&lt;h2&gt;Things I did in week 6&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Line number references&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Earlier, the bot used to reference the Python file directly. This made it difficult to search and find the particular function/class. We had to manually go and search. I modified the code to include a link with line numbers. Now the references section will give a link which wraps around the function/class. To do this I had to re-index the whole library again using the new parser code. The present model points to the latest stable release of FURY.&lt;/p&gt;
&lt;p&gt;I also tried to compress it all into one Discord message, reducing one extra ping :)&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;RAG Performance Evaluation&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-27-week-6-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;I added a new benchmark to measure RAG performance. It essentially checks whether certain key information was retrieved from the database. There are certain situations where the model fetches data irrelevant to the question, this could help in fixing that.&lt;/p&gt;
&lt;p&gt;The RAG benchmark dataset consists of a prompt to the LLM and expected references to be fetched from the database. I’ll give a score based on the % of correct fetches.&lt;/p&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Fine-tuning feasibility study&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-27-week-6-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “3” (ordinal 3)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;It was time to start thinking about fine-tuning. Gemini had a generous free tier and it was possible to fine-tune Gemini-1.0-pro. I looked into it and started collecting data for it. For fine-tuning Gemini, I had to format the data as an input/output pair. Most of the data were planned to be collected from Discord and GitHub.&lt;/p&gt;
&lt;p&gt;I also checked into fine-tuning models like phi-3 and llama 7b. It is possible to do the fine-tuning on google colab/kaggle. We could use a small quantized model and fine-tune that without much performance loss.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;p&gt;I’ll be taking a break next week due to my semester final examinations. I’ll study model finetuning and keep brainstorming interesting trajectories for FURY.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;No, I did not get stuck anywhere.&lt;/p&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-07-27-week-6-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 6.</summary>
    <category term="google" label="google"/>
    <published>2024-07-27T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-07-01-week-5-robin.html</id>
    <title>Week 5: LLM Benchmarking &amp; Architecture Modifications</title>
    <updated>2024-07-01T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-5-llm-benchmarking-architecture-modifications"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 5.&lt;/p&gt;
&lt;p&gt;This week, we’ll take all the things we did in the previous weeks, and quantify them. Benchmarking an LLM is the process of grading the LLM answer. To grade properly, we need good rubrics, so that’s what I worked on this week. Also, I made some architectural changes, to make the overall development simple.&lt;/p&gt;
&lt;section id="things-i-did-in-week-5"&gt;
&lt;h2&gt;Things I did in Week 5&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Architectural Update&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Earlier, this was our architecture:&lt;/p&gt;
&lt;blockquote&gt;
&lt;div&gt;&lt;img src="https://github.com/fury-gl/fury-communication-assets/blob/main/gsoc_2024/7-6-2024-demo-architecture-gsoc-robin-week2.png?raw=true"&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;p&gt;This had an obvious issue, all the core logic was inside the Discord Bot. So if I want to say, use the LLM inference for making a GitHub bot, or for benchmarking etc, it wasn’t possible. So I decided to cut the LLM logic from Discord Bot and made a new &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;LLM&lt;/span&gt; &lt;span class="pre"&gt;Router&lt;/span&gt;&lt;/code&gt;. It’ll handle all the LLM logic from now on, and we do not directly call any other endpoint other than this one.
It makes life simple, every input going into the endpoint goes like this:&lt;/p&gt;
&lt;div class="highlight-json notranslate"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="nt"&gt;&amp;quot;query&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Render a cube in fury&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;&amp;quot;llm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;llama3-70b-8192&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;&amp;quot;knn&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;&amp;quot;stream&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;false&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Every response coming out will be like this:&lt;/p&gt;
&lt;div class="highlight-json notranslate"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;response&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Yes, this is how it would be done python import fury....&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;references&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;1, 2, 3&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;dl class="simple"&gt;
&lt;dt&gt;What happens on the inside is completely abstracted away. You just call this and it’ll&lt;/dt&gt;&lt;dd&gt;&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;call the embedding model&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;pass embeddings to the database&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;return them to LLM (which you can choose)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;returns LLM answer with references to you&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Currently, we support &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;ollama&lt;/span&gt;&lt;/code&gt;, &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;google&lt;/span&gt;&lt;/code&gt; and &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;groq&lt;/span&gt;&lt;/code&gt; providers. That itself is 20+ LLM support, and you could swap between them using &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;/api/groq&lt;/span&gt; &lt;span class="pre"&gt;or&lt;/span&gt; &lt;span class="pre"&gt;api/google&lt;/span&gt; &lt;span class="pre"&gt;or&lt;/span&gt; &lt;span class="pre"&gt;/api/ollama&lt;/span&gt; &lt;span class="pre"&gt;...&lt;/span&gt;&lt;/code&gt;. Adding another provider is simply adding another endpoint.&lt;/p&gt;
&lt;p&gt;So if you do&lt;/p&gt;
&lt;p&gt;&lt;cite&gt;curl -X POST https://robinroy03-fury-engine.hf.space/api/groq/generate -H “Content-Type: application/json” -d ‘{“query”: “How do I create a sphere in FURY?”, “llm”: “llama3-70b-8192”, “knn”: “3”, “stream”: false}’&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;You’ll get a response from &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;llama3-70b-8192&lt;/span&gt;&lt;/code&gt; using &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;groq&lt;/span&gt;&lt;/code&gt;. If you do &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;https://robinroy03-fury-engine.hf.space/api/google/generate&lt;/span&gt;&lt;/code&gt; you can call any google gemini modes like &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;gemini-1.5-pro&lt;/span&gt;&lt;/code&gt; or &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;gemini-1.5-flash&lt;/span&gt;&lt;/code&gt;. Same for &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;ollama&lt;/span&gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This still could be improved, it does not currently account for vision models. I did not add that because we do not use vision models other than for benchmarking now, and that too is done locally. Benchmarking could also be streamlined, I avoided that because benchmarking is still in development so I’ll have to rewrite every day. Presently you can use this core &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;router&lt;/span&gt;&lt;/code&gt; for a working LLM generation (you’ll get the same thing you’ll get from the Discord Bot. So if you have a website, all you have to do is call the API).&lt;/p&gt;
&lt;p&gt;This is our present architecture:&lt;/p&gt;
&lt;img alt="Present LLM architecture." src="https://fury.gl/_images/gsoc_llm_robin_week5.jpg" /&gt;
&lt;p&gt;It is the same thing as above, except we have two new components - &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;LLM&lt;/span&gt; &lt;span class="pre"&gt;Engine&lt;/span&gt;&lt;/code&gt; and a &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Groq&lt;/span&gt; &lt;span class="pre"&gt;&amp;amp;&lt;/span&gt; &lt;span class="pre"&gt;Gemini&lt;/span&gt;&lt;/code&gt; endpoint. When we’ll end up having a conversational model setup (right now, it is one question and one answer), this model will be upgraded to accommodate that. My plan is to extend LLM Engine and add that. Other features such as vision also could be added to this as needed.&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Gemini Models added&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-01-week-5-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;As mentioned above, I added &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Gemini&lt;/span&gt;&lt;/code&gt; models this week. They have a decent free tier. Also, I’m studying the feasibility of fine-tuning using the &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Gemini&lt;/span&gt;&lt;/code&gt; models.&lt;/p&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;LLM Benchmarking&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-01-week-5-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “3” (ordinal 3)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;LLM Benchmarking is the process of evaluating the LLM output and giving a score. With this, making the model better will be simply a function of increasing the score. This area is still under development and the things I’ve tried here are the current standard procedures. To understand more about benchmarking, you can read: &lt;a class="reference external" href="https://huggingface.co/learn/cookbook/en/rag_evaluation"&gt;RAG Evaluation&lt;/a&gt;, &lt;a class="reference external" href="https://huggingface.co/learn/cookbook/en/llm_judge"&gt;Using LLM-as-a-judge 🧑‍⚖️ for an automated and versatile evaluation&lt;/a&gt; and &lt;a class="reference external" href="https://huggingface.co/learn/cookbook/en/advanced_rag"&gt;Advanced RAG on Hugging Face documentation using LangChain&lt;/a&gt;. This &lt;a class="reference external" href="https://www.deeplearning.ai/short-courses/advanced-retrieval-for-ai/"&gt;course&lt;/a&gt; is also amazing.&lt;/p&gt;
&lt;p&gt;I’ll anyways give a TL;DR:
LLM benchmarking is essentially like writing an English Literature exam and getting the grades. Your evaluator may give you a 4 or a 5, and the reasoning can be varied. For the same answer, you may even get very varied results from 2 different evaluators! Two common rubrics they use are &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;groundedness&lt;/span&gt; &lt;span class="pre"&gt;(whether&lt;/span&gt; &lt;span class="pre"&gt;the&lt;/span&gt; &lt;span class="pre"&gt;answer&lt;/span&gt; &lt;span class="pre"&gt;follows&lt;/span&gt; &lt;span class="pre"&gt;from&lt;/span&gt; &lt;span class="pre"&gt;the&lt;/span&gt; &lt;span class="pre"&gt;material)&lt;/span&gt;&lt;/code&gt; and &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;completion&lt;/span&gt; &lt;span class="pre"&gt;(whether&lt;/span&gt; &lt;span class="pre"&gt;the&lt;/span&gt; &lt;span class="pre"&gt;answer&lt;/span&gt; &lt;span class="pre"&gt;is&lt;/span&gt; &lt;span class="pre"&gt;complete,&lt;/span&gt; &lt;span class="pre"&gt;whether&lt;/span&gt; &lt;span class="pre"&gt;it&lt;/span&gt; &lt;span class="pre"&gt;fully&lt;/span&gt; &lt;span class="pre"&gt;answers&lt;/span&gt; &lt;span class="pre"&gt;the&lt;/span&gt; &lt;span class="pre"&gt;question&lt;/span&gt; &lt;span class="pre"&gt;with&lt;/span&gt; &lt;span class="pre"&gt;respect&lt;/span&gt; &lt;span class="pre"&gt;to&lt;/span&gt; &lt;span class="pre"&gt;the&lt;/span&gt; &lt;span class="pre"&gt;material)&lt;/span&gt;&lt;/code&gt;. These are the same rubrics we’ll use for LLM evaluation. For code, it’s different. The code should compile and do exactly what it should.&lt;/p&gt;
&lt;p&gt;Now FURY Bot does 2 things - writing code &amp;amp; writing answers for common questions (on GitHub issues etc). Presently, I’ve only collected data for coding questions, as they are much easier to evaluate and give a clear sense of direction (also I found more coding data).&lt;/p&gt;
&lt;dl class="simple"&gt;
&lt;dt&gt;Evaluating FURY code can be done by:&lt;/dt&gt;&lt;dd&gt;&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;Running the code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Checking the output.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Now we do this using &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;pytest&lt;/span&gt;&lt;/code&gt; in the FURY repo for tests. But this approach is tedious, as collecting questions and writing test cases take a lot of time, also the orientation of the 3D objects also matters (an LLM generation is not deterministic). So we are using a vision model &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;moondream2&lt;/span&gt;&lt;/code&gt; to check the LLM generated output and verify if it is what we actually wanted.
On a high level, this is what we do (for now):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Take a QnA pair from the collected dataset (I’ve collected ~23 questions).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ask the LLM to generate a FURY code for that (using the references).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run this generated code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Check the output using &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;moondream2&lt;/span&gt;&lt;/code&gt; and verify whether it is what we wanted.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is also &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;fast_eval&lt;/span&gt;&lt;/code&gt; which checks whether the code compiles and skips &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;moondream2&lt;/span&gt;&lt;/code&gt; entirely. This is obviously faster and is also decently good (is actually a pretty good heuristic). If it runs, assume it works :)&lt;/p&gt;
&lt;p&gt;This is our current stats: (from now on, we can finally talk using numbers)&lt;/p&gt;
&lt;section id="coding-benchmark"&gt;
&lt;h3&gt;Coding benchmark:&lt;/h3&gt;
&lt;p&gt;On &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;fast_eval&lt;/span&gt;&lt;/code&gt; we have a success rate of &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;47.83%&lt;/span&gt;&lt;/code&gt; for &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;groq&lt;/span&gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;normal_eval&lt;/span&gt;&lt;/code&gt; we have a success rate of &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;13.04%&lt;/span&gt;&lt;/code&gt; for &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;groq&lt;/span&gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Note that &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;moondream2&lt;/span&gt;&lt;/code&gt; also sometimes mistakes the output for something else. It is close to &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;~45%&lt;/span&gt;&lt;/code&gt; when I checked manually. For now, I’m only going to focus on &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;fast_eval&lt;/span&gt;&lt;/code&gt; as fixing &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;moondream2&lt;/span&gt;&lt;/code&gt; is a distraction for the moment. (This actually gets very meta, there are projects where they have benchmarks for the evaluator and so on. &lt;a class="reference external" href="https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/"&gt;Read this&lt;/a&gt;.)&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Better benchmark scores :)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Line number highlighting &amp;#64; references.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;references&lt;/span&gt;&lt;/code&gt; improvements.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;No, I did not get stuck anywhere.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-01-week-5-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id1"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “rag evaluation”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/learn/cookbook/en/rag_evaluation"&gt;RAG Evaluation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/learn/cookbook/en/llm_judge"&gt;LLM Judge&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/learn/cookbook/en/advanced_rag"&gt;Advanced RAG&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://www.deeplearning.ai/short-courses/advanced-retrieval-for-ai/"&gt;Advanced Retrieval for AI&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/vikhyatk/moondream2"&gt;Moondream2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/"&gt;Finding GPT-4 mistakes with GPT-4&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-07-01-week-5-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 5.</summary>
    <category term="google" label="google"/>
    <published>2024-07-01T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-07-01-week-4-robin.html</id>
    <title>Week 4: Pipeline Improvements and Taking The Bot Public!</title>
    <updated>2024-07-01T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-4-pipeline-improvements-and-taking-the-bot-public"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 4.&lt;/p&gt;
&lt;p&gt;My goals for week 4 were to move my Google colab notes to a proper Python script, improve the existing code, and make a working pipeline to upsert data easily. Also, the bot is public now :) Anyone reading this blog could join this &lt;a class="reference external" href="https://discord.gg/NVkE6Qd2bZ"&gt;Discord Server&lt;/a&gt; and ask questions right away!&lt;/p&gt;
&lt;section id="things-i-did-in-week-4"&gt;
&lt;h2&gt;Things I did in Week 4&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Chunking tutorials and documentation&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Earlier, only files fitting the context window of the embedding model were upserted. This was because otherwise, we’d have to split the file in half and lose the overall context. This will lead to information loss and retrieval will be messy. Now, I decided I’d upsert everything by splitting information properly. By “properly”, what I mean is it won’t be a random split, and there’ll be logical reasoning behind every chunk.&lt;/p&gt;
&lt;p&gt;This area is still actively studied, and the whole concept is to find ideal chunks which are self-sufficient and contain the most information. This &lt;a class="reference external" href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb"&gt;notebook&lt;/a&gt; details 6 different approaches, I read through them and some of their associated literature and decided we’ll use &lt;cite&gt;Recursive Character Text Splitting&lt;/cite&gt; and &lt;cite&gt;Document Specific Splitting&lt;/cite&gt; for now. There is no major reason for this, I just felt it’ll work well for now (a reasoning-backed approach will come in a few weeks). There is a lot of experimentation we could do here, a better chunking will result in better &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;references&lt;/span&gt;&lt;/code&gt; generation and so on.&lt;/p&gt;
&lt;dl class="simple"&gt;
&lt;dt&gt;So this is our current process&lt;/dt&gt;&lt;dd&gt;&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;if normal function/class definition: no splitting, chunk as it is.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if rst files, use the &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;rst&lt;/span&gt; &lt;span class="pre"&gt;parser&lt;/span&gt;&lt;/code&gt; and split them with a chunk size of ~8000 tokens (max llama could take). RST files in FURY contain documentation &amp;amp; blog posts.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if tutorial, try chunking as it is, if not possible split at 8000 tokens.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Function/class definitions are generally under 8000 so I’ve not done explicit checks for now, the model will trim the remaining if longer (I found some long classes later).&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Move colab files to a proper Python script&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-01-week-4-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;I did all the upsertion and experiments on colab. It is messy and can’t be used in production. We need a one-click approach to upsertion. Something like point to &lt;cite&gt;fury&lt;/cite&gt; directory and it should do everything. So I took the messy colab code and made a python script from it.&lt;/p&gt;
&lt;p&gt;One of my key goals is to separate core application logic from LLMs/Database providers. We should be able to swap them as needed without much fuss. I’ll talk more about this in week 5.&lt;/p&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Taking the bot public!&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-01-week-4-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “3” (ordinal 3)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;The whole point of making the bot is to help improve the productivity of FURY developers. So I decided to take it public on &lt;a class="reference external" href="https://discord.gg/NVkE6Qd2bZ"&gt;this discord server&lt;/a&gt;. You could use it today! (actually, you could’ve used it from the 20th of last month, this blog got delayed😢)&lt;/p&gt;
&lt;p&gt;I’ll observe what people are asking and then iterate towards making the bot better in that area. I think it’ll be better than making the bot good on what I believe is the best.&lt;/p&gt;
&lt;ol class="arabic simple" start="4"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Minor bugfixes and stuff&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-01-week-4-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “4” (ordinal 4)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;Did some minor bug fixes on things like the Discord bot generation cutoff and error handling improvements. It was Discord message limit (&amp;lt;=2000) that caused the generation to cut off, I split the message into parts to fix that. Error handling was improved generally everywhere. I’ll need to bring logging later.&lt;/p&gt;
&lt;section id="minor-sidequest"&gt;
&lt;h3&gt;Minor Sidequest&lt;/h3&gt;
&lt;p&gt;This is in no way related to FURY, but it was fun so I thought I’d add it here :)&lt;/p&gt;
&lt;p&gt;So after midterms, I decided to go back home, to maximally use my time I searched for things to do and found a local FOSS event: (&lt;a class="reference external" href="https://x.com/FOSSUnitedKochi/status/1804763181274759645"&gt;Kochi’s FOSS&lt;/a&gt;). It was done by FOSS United Kochi and it’s one of the major FOSS events in my state (Kerala, India). Met some Pythonistas! Explained what FURY is to them. I also ended up finding some lore (&lt;a class="reference external" href="https://www.gnu.org/education/edu-system-india.html"&gt;click here to read&lt;/a&gt;) about how GNU/Linux spread in Kerala, India. Also found some old FOSS event pictures (&lt;a class="reference external" href="https://www.flickr.com/photos/pce/245170427/in/photostream/"&gt;this&lt;/a&gt; one is talking about Python, 2003 World of Python). This was my first FOSS event outside campus so it was fun :)&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Benchmarking&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Architecture Update&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;No, I did not get stuck. This week was more of learning and experimentation so I think it’s normal what I encountered.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-01-week-4-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id1"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “discord server”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://discord.gg/NVkE6Qd2bZ"&gt;Discord Server&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb"&gt;A Text Splitting Guide&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://www.gnu.org/education/edu-system-india.html"&gt;GNU Case of Kerala&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://www.flickr.com/photos/pce/245170427/in/photostream/"&gt;2003 World of Python&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://x.com/FOSSUnitedKochi/status/1804763181274759645"&gt;FOSS United Kochi&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin :)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-07-01-week-4-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 4.</summary>
    <category term="google" label="google"/>
    <published>2024-07-01T00:00:00+00:00</published>
  </entry>
</feed>
