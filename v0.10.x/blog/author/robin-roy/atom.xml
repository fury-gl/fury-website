<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://fury.gl/</id>
  <title>Blog - Posts by Robin Roy</title>
  <updated>2024-07-31T18:42:04.382004+00:00</updated>
  <link href="https://fury.gl/"/>
  <link href="https://fury.gl/blog/author/robin-roy/atom.xml" rel="self"/>
  <generator uri="https://ablog.readthedocs.io/" version="0.11.10">ABlog</generator>
  <entry>
    <id>https://fury.gl/posts/2024/2024-07-27-week8-robin.html</id>
    <title>Week 8: Gemini Finetuning</title>
    <updated>2024-07-27T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-8-gemini-finetuning"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 8.&lt;/p&gt;
&lt;p&gt;This week I worked on finalizing the Discord chat QnA data collection and using it to Fine-Tune the Gemini-1.0-Pro model.&lt;/p&gt;
&lt;section id="things-i-did-in-week-8"&gt;
&lt;h2&gt;Things I did in Week 8&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Discord Data Collection&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I finished collecting data from all the channels in the Discord server, cross-verifying to check whether they still work. I also added some questions which were on the FURY bot testing server. These QnA pairs were later converted to a CSV with input/output pairs and fed to Gemini for finetuning.&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Gemini Finetuning&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-27-week8-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;Finetuning is essentially training the model on the input/output. RAG is giving context and asking the model to form an answer using that. Finetuning updates the model weights as per the input/output. Gemini uses &lt;a class="reference external" href="https://huggingface.co/blog/peft"&gt;Parameter-Efficient Fine-Tuning&lt;/a&gt; in AI Studio as per some reports. It makes sense because the tuning only takes minutes and PEFT is a good strategy to prevent issues like &lt;a class="reference external" href="https://arxiv.org/abs/1312.6211"&gt;catastrophic forgetting&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finetuning and RAG are complementary to each other. The difference between them can be summarized as follows:&lt;/p&gt;
&lt;p&gt;RAG is like giving an LLM with no prior knowledge about FURY access to some important functions/classes as per the user prompt. It’ll use this given context and its knowledge of graphics libraries (knowledge from pretraining) to form an answer.&lt;/p&gt;
&lt;p&gt;Finetuning is used to make the model follow a certain style or behaviour. It is a form of mimicking the input-output. This will help in increasing the model’s performance. An interesting thing is I had to train the model 1) with RAG and 2) without RAG.&lt;/p&gt;
&lt;p&gt;For finetuning, the input must be in the format the LLM will get the answer from the user. When you ask a question to the FURY bot, the bot does not get your question directly. We are processing it to add additional information. Therefore I had to process all the collected data with RAG.&lt;/p&gt;
&lt;p&gt;This is an interesting direction, and I have a lot of cool things to try out here. I’ll spend the next few weeks trying different ideas.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Finetuning strategies.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hosting the model on API.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;No, I did not get stuck anywhere.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-27-week8-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id1"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “parameter-efficient fine-tuning”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/blog/peft"&gt;Parameter-Efficient Fine-Tuning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-27-week8-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id2"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “catastrophic forgetting”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://arxiv.org/abs/1312.6211"&gt;catastrophic forgetting&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-07-27-week8-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 8.</summary>
    <category term="google" label="google"/>
    <published>2024-07-27T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-07-27-week7-robin.html</id>
    <title>Week 7: Surviving final examinations</title>
    <updated>2024-07-27T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-7-surviving-final-examinations"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 7.&lt;/p&gt;
&lt;p&gt;I majorly took this week off due to my semester final examinations :) They were fun. Major topics were x86, ARM and 8051. I had not written a lot of assembly apart from school work. I took the week to experiment with some assembly. The course was more into hardware architecture than programming. I’ve now enough knowledge to read a given piece of ASM code with a wiki to look up mnemonics (and Gemini/Claude to help). I’m not fast in writing ASM (yet), one day I’ll find a project to dive into, or maybe some reverse engineering and CTFs. GPU instruction sets are also something interesting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Discord data collection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I collected some Q&amp;amp;A questions from the FURY discord server. I did it manually because the volume wasn’t high, and I wanted it to be correct. Had to cross-check with GitHub also to check whether the answer/code mentioned still stands. The format I used was [User question, Answer]. If the answer/question is spread across multiple conversations, I’ll adjust it to this format.&lt;/p&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Gemini Finetuning&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Collect more Discord data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;Not really apart from some silly ASM bugs.&lt;/p&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-07-27-week7-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 7.</summary>
    <category term="google" label="google"/>
    <published>2024-07-27T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-07-27-week6-robin.html</id>
    <title>Week 6: UI Improvements and RAG performance evaluation</title>
    <updated>2024-07-27T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-6-ui-improvements-and-rag-performance-evaluation"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 6.&lt;/p&gt;
&lt;p&gt;This week, I worked on some UI improvements and studied and evaluated the RAG performance.&lt;/p&gt;
&lt;section id="things-i-did-in-week-6"&gt;
&lt;h2&gt;Things I did in week 6&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Line number references&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Earlier, the bot used to reference the Python file directly. This made it difficult to search and find the particular function/class. We had to manually go and search. I modified the code to include a link with line numbers. Now the references section will give a link which wraps around the function/class. To do this I had to re-index the whole library again using the new parser code. The present model points to the latest stable release of FURY.&lt;/p&gt;
&lt;p&gt;I also tried to compress it all into one Discord message, reducing one extra ping :)&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;RAG Performance Evaluation&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-27-week6-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;I added a new benchmark to measure RAG performance. It essentially checks whether certain key information was retrieved from the database. There are certain situations where the model fetches data irrelevant to the question, this could help in fixing that.&lt;/p&gt;
&lt;p&gt;The RAG benchmark dataset consists of a prompt to the LLM and expected references to be fetched from the database. I’ll give a score based on the % of correct fetches.&lt;/p&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Fine-tuning feasibility study&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-27-week6-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “3” (ordinal 3)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;It was time to start thinking about fine-tuning. Gemini had a generous free tier and it was possible to fine-tune Gemini-1.0-pro. I looked into it and started collecting data for it. For fine-tuning Gemini, I had to format the data as an input/output pair. Most of the data were planned to be collected from Discord and GitHub.&lt;/p&gt;
&lt;p&gt;I also checked into fine-tuning models like phi-3 and llama 7b. It is possible to do the fine-tuning on google colab/kaggle. We could use a small quantized model and fine-tune that without much performance loss.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;p&gt;I’ll be taking a break next week due to my semester final examinations. I’ll study model finetuning and keep brainstorming interesting trajectories for FURY.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;No, I did not get stuck anywhere.&lt;/p&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-07-27-week6-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 6.</summary>
    <category term="google" label="google"/>
    <published>2024-07-27T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-07-01-week-5-robin.html</id>
    <title>Week 5: LLM Benchmarking &amp; Architecture Modifications</title>
    <updated>2024-07-01T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-5-llm-benchmarking-architecture-modifications"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 5.&lt;/p&gt;
&lt;p&gt;This week, we’ll take all the things we did in the previous weeks, and quantify them. Benchmarking an LLM is the process of grading the LLM answer. To grade properly, we need good rubrics, so that’s what I worked on this week. Also, I made some architectural changes, to make the overall development simple.&lt;/p&gt;
&lt;section id="things-i-did-in-week-5"&gt;
&lt;h2&gt;Things I did in Week 5&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Architectural Update&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Earlier, this was our architecture:&lt;/p&gt;
&lt;blockquote&gt;
&lt;div&gt;&lt;img src="https://github.com/fury-gl/fury-communication-assets/blob/main/gsoc_2024/7-6-2024-demo-architecture-gsoc-robin-week2.png?raw=true"&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;p&gt;This had an obvious issue, all the core logic was inside the Discord Bot. So if I want to say, use the LLM inference for making a GitHub bot, or for benchmarking etc, it wasn’t possible. So I decided to cut the LLM logic from Discord Bot and made a new &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;LLM&lt;/span&gt; &lt;span class="pre"&gt;Router&lt;/span&gt;&lt;/code&gt;. It’ll handle all the LLM logic from now on, and we do not directly call any other endpoint other than this one.
It makes life simple, every input going into the endpoint goes like this:&lt;/p&gt;
&lt;div class="highlight-json notranslate"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="nt"&gt;&amp;quot;query&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Render a cube in fury&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;&amp;quot;llm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;llama3-70b-8192&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;&amp;quot;knn&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="nt"&gt;&amp;quot;stream&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;false&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Every response coming out will be like this:&lt;/p&gt;
&lt;div class="highlight-json notranslate"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;response&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Yes, this is how it would be done python import fury....&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;references&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;1, 2, 3&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;dl class="simple"&gt;
&lt;dt&gt;What happens on the inside is completely abstracted away. You just call this and it’ll&lt;/dt&gt;&lt;dd&gt;&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;call the embedding model&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;pass embeddings to the database&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;return them to LLM (which you can choose)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;returns LLM answer with references to you&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Currently, we support &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;ollama&lt;/span&gt;&lt;/code&gt;, &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;google&lt;/span&gt;&lt;/code&gt; and &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;groq&lt;/span&gt;&lt;/code&gt; providers. That itself is 20+ LLM support, and you could swap between them using &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;/api/groq&lt;/span&gt; &lt;span class="pre"&gt;or&lt;/span&gt; &lt;span class="pre"&gt;api/google&lt;/span&gt; &lt;span class="pre"&gt;or&lt;/span&gt; &lt;span class="pre"&gt;/api/ollama&lt;/span&gt; &lt;span class="pre"&gt;...&lt;/span&gt;&lt;/code&gt;. Adding another provider is simply adding another endpoint.&lt;/p&gt;
&lt;p&gt;So if you do&lt;/p&gt;
&lt;p&gt;&lt;cite&gt;curl -X POST https://robinroy03-fury-engine.hf.space/api/groq/generate -H “Content-Type: application/json” -d ‘{“query”: “How do I create a sphere in FURY?”, “llm”: “llama3-70b-8192”, “knn”: “3”, “stream”: false}’&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;You’ll get a response from &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;llama3-70b-8192&lt;/span&gt;&lt;/code&gt; using &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;groq&lt;/span&gt;&lt;/code&gt;. If you do &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;https://robinroy03-fury-engine.hf.space/api/google/generate&lt;/span&gt;&lt;/code&gt; you can call any google gemini modes like &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;gemini-1.5-pro&lt;/span&gt;&lt;/code&gt; or &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;gemini-1.5-flash&lt;/span&gt;&lt;/code&gt;. Same for &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;ollama&lt;/span&gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This still could be improved, it does not currently account for vision models. I did not add that because we do not use vision models other than for benchmarking now, and that too is done locally. Benchmarking could also be streamlined, I avoided that because benchmarking is still in development so I’ll have to rewrite every day. Presently you can use this core &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;router&lt;/span&gt;&lt;/code&gt; for a working LLM generation (you’ll get the same thing you’ll get from the Discord Bot. So if you have a website, all you have to do is call the API).&lt;/p&gt;
&lt;p&gt;This is our present architecture:&lt;/p&gt;
&lt;img alt="Present LLM architecture." src="https://fury.gl/_images/gsoc_llm_robin_week5.jpg" /&gt;
&lt;p&gt;It is the same thing as above, except we have two new components - &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;LLM&lt;/span&gt; &lt;span class="pre"&gt;Engine&lt;/span&gt;&lt;/code&gt; and a &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Groq&lt;/span&gt; &lt;span class="pre"&gt;&amp;amp;&lt;/span&gt; &lt;span class="pre"&gt;Gemini&lt;/span&gt;&lt;/code&gt; endpoint. When we’ll end up having a conversational model setup (right now, it is one question and one answer), this model will be upgraded to accommodate that. My plan is to extend LLM Engine and add that. Other features such as vision also could be added to this as needed.&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Gemini Models added&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-01-week-5-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;As mentioned above, I added &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Gemini&lt;/span&gt;&lt;/code&gt; models this week. They have a decent free tier. Also, I’m studying the feasibility of fine-tuning using the &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;Gemini&lt;/span&gt;&lt;/code&gt; models.&lt;/p&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;LLM Benchmarking&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-01-week-5-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “3” (ordinal 3)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;LLM Benchmarking is the process of evaluating the LLM output and giving a score. With this, making the model better will be simply a function of increasing the score. This area is still under development and the things I’ve tried here are the current standard procedures. To understand more about benchmarking, you can read: &lt;a class="reference external" href="https://huggingface.co/learn/cookbook/en/rag_evaluation"&gt;RAG Evaluation&lt;/a&gt;, &lt;a class="reference external" href="https://huggingface.co/learn/cookbook/en/llm_judge"&gt;Using LLM-as-a-judge 🧑‍⚖️ for an automated and versatile evaluation&lt;/a&gt; and &lt;a class="reference external" href="https://huggingface.co/learn/cookbook/en/advanced_rag"&gt;Advanced RAG on Hugging Face documentation using LangChain&lt;/a&gt;. This &lt;a class="reference external" href="https://www.deeplearning.ai/short-courses/advanced-retrieval-for-ai/"&gt;course&lt;/a&gt; is also amazing.&lt;/p&gt;
&lt;p&gt;I’ll anyways give a TL;DR:
LLM benchmarking is essentially like writing an English Literature exam and getting the grades. Your evaluator may give you a 4 or a 5, and the reasoning can be varied. For the same answer, you may even get very varied results from 2 different evaluators! Two common rubrics they use are &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;groundedness&lt;/span&gt; &lt;span class="pre"&gt;(whether&lt;/span&gt; &lt;span class="pre"&gt;the&lt;/span&gt; &lt;span class="pre"&gt;answer&lt;/span&gt; &lt;span class="pre"&gt;follows&lt;/span&gt; &lt;span class="pre"&gt;from&lt;/span&gt; &lt;span class="pre"&gt;the&lt;/span&gt; &lt;span class="pre"&gt;material)&lt;/span&gt;&lt;/code&gt; and &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;completion&lt;/span&gt; &lt;span class="pre"&gt;(whether&lt;/span&gt; &lt;span class="pre"&gt;the&lt;/span&gt; &lt;span class="pre"&gt;answer&lt;/span&gt; &lt;span class="pre"&gt;is&lt;/span&gt; &lt;span class="pre"&gt;complete,&lt;/span&gt; &lt;span class="pre"&gt;whether&lt;/span&gt; &lt;span class="pre"&gt;it&lt;/span&gt; &lt;span class="pre"&gt;fully&lt;/span&gt; &lt;span class="pre"&gt;answers&lt;/span&gt; &lt;span class="pre"&gt;the&lt;/span&gt; &lt;span class="pre"&gt;question&lt;/span&gt; &lt;span class="pre"&gt;with&lt;/span&gt; &lt;span class="pre"&gt;respect&lt;/span&gt; &lt;span class="pre"&gt;to&lt;/span&gt; &lt;span class="pre"&gt;the&lt;/span&gt; &lt;span class="pre"&gt;material)&lt;/span&gt;&lt;/code&gt;. These are the same rubrics we’ll use for LLM evaluation. For code, it’s different. The code should compile and do exactly what it should.&lt;/p&gt;
&lt;p&gt;Now FURY Bot does 2 things - writing code &amp;amp; writing answers for common questions (on GitHub issues etc). Presently, I’ve only collected data for coding questions, as they are much easier to evaluate and give a clear sense of direction (also I found more coding data).&lt;/p&gt;
&lt;dl class="simple"&gt;
&lt;dt&gt;Evaluating FURY code can be done by:&lt;/dt&gt;&lt;dd&gt;&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;Running the code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Checking the output.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Now we do this using &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;pytest&lt;/span&gt;&lt;/code&gt; in the FURY repo for tests. But this approach is tedious, as collecting questions and writing test cases take a lot of time, also the orientation of the 3D objects also matters (an LLM generation is not deterministic). So we are using a vision model &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;moondream2&lt;/span&gt;&lt;/code&gt; to check the LLM generated output and verify if it is what we actually wanted.
On a high level, this is what we do (for now):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Take a QnA pair from the collected dataset (I’ve collected ~23 questions).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ask the LLM to generate a FURY code for that (using the references).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run this generated code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Check the output using &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;moondream2&lt;/span&gt;&lt;/code&gt; and verify whether it is what we wanted.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is also &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;fast_eval&lt;/span&gt;&lt;/code&gt; which checks whether the code compiles and skips &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;moondream2&lt;/span&gt;&lt;/code&gt; entirely. This is obviously faster and is also decently good (is actually a pretty good heuristic). If it runs, assume it works :)&lt;/p&gt;
&lt;p&gt;This is our current stats: (from now on, we can finally talk using numbers)&lt;/p&gt;
&lt;section id="coding-benchmark"&gt;
&lt;h3&gt;Coding benchmark:&lt;/h3&gt;
&lt;p&gt;On &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;fast_eval&lt;/span&gt;&lt;/code&gt; we have a success rate of &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;47.83%&lt;/span&gt;&lt;/code&gt; for &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;groq&lt;/span&gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;normal_eval&lt;/span&gt;&lt;/code&gt; we have a success rate of &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;13.04%&lt;/span&gt;&lt;/code&gt; for &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;groq&lt;/span&gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Note that &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;moondream2&lt;/span&gt;&lt;/code&gt; also sometimes mistakes the output for something else. It is close to &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;~45%&lt;/span&gt;&lt;/code&gt; when I checked manually. For now, I’m only going to focus on &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;fast_eval&lt;/span&gt;&lt;/code&gt; as fixing &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;moondream2&lt;/span&gt;&lt;/code&gt; is a distraction for the moment. (This actually gets very meta, there are projects where they have benchmarks for the evaluator and so on. &lt;a class="reference external" href="https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/"&gt;Read this&lt;/a&gt;.)&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Better benchmark scores :)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Line number highlighting &amp;#64; references.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;references&lt;/span&gt;&lt;/code&gt; improvements.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;No, I did not get stuck anywhere.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-01-week-5-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id1"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “rag evaluation”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/learn/cookbook/en/rag_evaluation"&gt;RAG Evaluation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/learn/cookbook/en/llm_judge"&gt;LLM Judge&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/learn/cookbook/en/advanced_rag"&gt;Advanced RAG&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://www.deeplearning.ai/short-courses/advanced-retrieval-for-ai/"&gt;Advanced Retrieval for AI&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/vikhyatk/moondream2"&gt;Moondream2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/"&gt;Finding GPT-4 mistakes with GPT-4&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-07-01-week-5-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 5.</summary>
    <category term="google" label="google"/>
    <published>2024-07-01T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-07-01-week-4-robin.html</id>
    <title>Week 4: Pipeline Improvements and Taking The Bot Public!</title>
    <updated>2024-07-01T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-4-pipeline-improvements-and-taking-the-bot-public"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 4.&lt;/p&gt;
&lt;p&gt;My goals for week 4 were to move my Google colab notes to a proper Python script, improve the existing code, and make a working pipeline to upsert data easily. Also, the bot is public now :) Anyone reading this blog could join this &lt;a class="reference external" href="https://discord.gg/NVkE6Qd2bZ"&gt;Discord Server&lt;/a&gt; and ask questions right away!&lt;/p&gt;
&lt;section id="things-i-did-in-week-4"&gt;
&lt;h2&gt;Things I did in Week 4&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Chunking tutorials and documentation&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Earlier, only files fitting the context window of the embedding model were upserted. This was because otherwise, we’d have to split the file in half and lose the overall context. This will lead to information loss and retrieval will be messy. Now, I decided I’d upsert everything by splitting information properly. By “properly”, what I mean is it won’t be a random split, and there’ll be logical reasoning behind every chunk.&lt;/p&gt;
&lt;p&gt;This area is still actively studied, and the whole concept is to find ideal chunks which are self-sufficient and contain the most information. This &lt;a class="reference external" href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb"&gt;notebook&lt;/a&gt; details 6 different approaches, I read through them and some of their associated literature and decided we’ll use &lt;cite&gt;Recursive Character Text Splitting&lt;/cite&gt; and &lt;cite&gt;Document Specific Splitting&lt;/cite&gt; for now. There is no major reason for this, I just felt it’ll work well for now (a reasoning-backed approach will come in a few weeks). There is a lot of experimentation we could do here, a better chunking will result in better &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;references&lt;/span&gt;&lt;/code&gt; generation and so on.&lt;/p&gt;
&lt;dl class="simple"&gt;
&lt;dt&gt;So this is our current process&lt;/dt&gt;&lt;dd&gt;&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;if normal function/class definition: no splitting, chunk as it is.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if rst files, use the &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;rst&lt;/span&gt; &lt;span class="pre"&gt;parser&lt;/span&gt;&lt;/code&gt; and split them with a chunk size of ~8000 tokens (max llama could take). RST files in FURY contain documentation &amp;amp; blog posts.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if tutorial, try chunking as it is, if not possible split at 8000 tokens.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Function/class definitions are generally under 8000 so I’ve not done explicit checks for now, the model will trim the remaining if longer (I found some long classes later).&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Move colab files to a proper Python script&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-01-week-4-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;I did all the upsertion and experiments on colab. It is messy and can’t be used in production. We need a one-click approach to upsertion. Something like point to &lt;cite&gt;fury&lt;/cite&gt; directory and it should do everything. So I took the messy colab code and made a python script from it.&lt;/p&gt;
&lt;p&gt;One of my key goals is to separate core application logic from LLMs/Database providers. We should be able to swap them as needed without much fuss. I’ll talk more about this in week 5.&lt;/p&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Taking the bot public!&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-01-week-4-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “3” (ordinal 3)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;The whole point of making the bot is to help improve the productivity of FURY developers. So I decided to take it public on &lt;a class="reference external" href="https://discord.gg/NVkE6Qd2bZ"&gt;this discord server&lt;/a&gt;. You could use it today! (actually, you could’ve used it from the 20th of last month, this blog got delayed😢)&lt;/p&gt;
&lt;p&gt;I’ll observe what people are asking and then iterate towards making the bot better in that area. I think it’ll be better than making the bot good on what I believe is the best.&lt;/p&gt;
&lt;ol class="arabic simple" start="4"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Minor bugfixes and stuff&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-01-week-4-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “4” (ordinal 4)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;Did some minor bug fixes on things like the Discord bot generation cutoff and error handling improvements. It was Discord message limit (&amp;lt;=2000) that caused the generation to cut off, I split the message into parts to fix that. Error handling was improved generally everywhere. I’ll need to bring logging later.&lt;/p&gt;
&lt;section id="minor-sidequest"&gt;
&lt;h3&gt;Minor Sidequest&lt;/h3&gt;
&lt;p&gt;This is in no way related to FURY, but it was fun so I thought I’d add it here :)&lt;/p&gt;
&lt;p&gt;So after midterms, I decided to go back home, to maximally use my time I searched for things to do and found a local FOSS event: (&lt;a class="reference external" href="https://x.com/FOSSUnitedKochi/status/1804763181274759645"&gt;Kochi’s FOSS&lt;/a&gt;). It was done by FOSS United Kochi and it’s one of the major FOSS events in my state (Kerala, India). Met some Pythonistas! Explained what FURY is to them. I also ended up finding some lore (&lt;a class="reference external" href="https://www.gnu.org/education/edu-system-india.html"&gt;click here to read&lt;/a&gt;) about how GNU/Linux spread in Kerala, India. Also found some old FOSS event pictures (&lt;a class="reference external" href="https://www.flickr.com/photos/pce/245170427/in/photostream/"&gt;this&lt;/a&gt; one is talking about Python, 2003 World of Python). This was my first FOSS event outside campus so it was fun :)&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Benchmarking&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Architecture Update&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;No, I did not get stuck. This week was more of learning and experimentation so I think it’s normal what I encountered.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-07-01-week-4-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id1"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “discord server”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://discord.gg/NVkE6Qd2bZ"&gt;Discord Server&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb"&gt;A Text Splitting Guide&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://www.gnu.org/education/edu-system-india.html"&gt;GNU Case of Kerala&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://www.flickr.com/photos/pce/245170427/in/photostream/"&gt;2003 World of Python&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://x.com/FOSSUnitedKochi/status/1804763181274759645"&gt;FOSS United Kochi&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin :)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-07-01-week-4-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 4.</summary>
    <category term="google" label="google"/>
    <published>2024-07-01T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-06-16-week3-robin.html</id>
    <title>Week 3: Data Data Data!</title>
    <updated>2024-06-16T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-3-data-data-data"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 3.&lt;/p&gt;
&lt;p&gt;My goal for week 3 was to collect data more efficiently and improve the citations. I also had my mid-terms during this week so I had to get things done fast.&lt;/p&gt;
&lt;section id="things-i-did-in-week-3"&gt;
&lt;h2&gt;Things I did in week 3&lt;/h2&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;A better data parsing technique&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;My initial approach was naive, it was just regex and some common filtrations. Later, my mentors told me to use the &lt;cite&gt;inspect&lt;/cite&gt; module. I studied that module and realized that I needed to parse data using an AST. I didn’t use the &lt;cite&gt;inspect&lt;/cite&gt; module to do the parsing, since I only had to get the function/class signature and docstrings. So instead I used the &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;ast&lt;/span&gt;&lt;/code&gt; module from python stdlib. My mentors gave me the general direction to go through - which was using ASTs to parse data effectively.&lt;/p&gt;
&lt;p&gt;So now we have a script which you run like &lt;cite&gt;python extractor.py fury&lt;/cite&gt; and it’ll generate the appropriate JSON files.&lt;/p&gt;
&lt;p&gt;&lt;cite&gt;{“path”: “../..”, “function/class name”: “name”, “docstring”: “..”, “class_methods”: [“method1”, “…”]}&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;I also changed the upserting chunk format. Earlier it was just strings, now it is JSON (same thing above). I do not have a scientific reason for this, but empirically it looks like it helped. Benchmarking is something I’m planning to do next week.&lt;/p&gt;
&lt;p&gt;Metadata format:&lt;/p&gt;
&lt;p&gt;&lt;cite&gt;metadata: {“path”: “../..”, “function/class name”: “name”, “docstring”: “..”, “methods”: [(method1, docstring), (method2, docstring), …]}&lt;/cite&gt;&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Links for citation&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-16-week3-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;Now the bot shows links for citations. Because of the new parsing, I was able to do that pretty efficiently.&lt;/p&gt;
&lt;img alt="Link based references for the LLM output." src="https://fury.gl/_images/gsoc-robin-3-fury-discord-bot-references-url.jpg" /&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Faster Inference&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-16-week3-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “3” (ordinal 3)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;So this is something about the Generative AI field. There are too many things happening you might miss some stuff. &lt;cite&gt;Groq&lt;/cite&gt; is a company providing free APIs for the llama and other opensource models (free for now, at least). Its inference speed is also super high. So I decided to integrate that also into our infrastructure.
Since everything is a microservice in our architecture, it is easy to add new things.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;Our architecture:&lt;/dt&gt;&lt;dd&gt;&lt;img src="https://github.com/fury-gl/fury-communication-assets/blob/main/gsoc_2024/7-6-2024-demo-architecture-gsoc-robin-week2.png?raw=true"&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;So now, along with Ollama, we have Groq inference also. I aim to make a &lt;cite&gt;router&lt;/cite&gt; so that we can swap different providers as required. I’m also very interested in integrating Google Gemini 1.5 Flash and other models. Groq does not support fine-tuning, but Flash supports it and is &lt;a class="reference external" href="https://developers.googleblog.com/en/gemini-15-pro-and-15-flash-now-available/#:~:text=To%20support%20that%2C%20we%20will%20also%20be%20rolling%20out%20tuning%20support%20for%20Gemini%201.5%20Flash%20on%20June%2017th.%20Tuning%20will%20be%20supported%20in%20both%20Google%20AI%20Studio%20and%20the%20Gemini%20API%20directly.%20Currently%2C%20tuning%20jobs%20are%20free%20of%20charge%2C%20and%20using%20a%20tuned%20model%20does%20not%20incur%20any%20additional%20per%2Dtoken%20costs."&gt;free of cost&lt;/a&gt; (for now). Our architecture is platform agnostic, so we can try out different things without being locked into any of them. We will also fine-tune our phi3 model since we have the data with us.&lt;/p&gt;
&lt;blockquote&gt;
&lt;div&gt;&lt;iframe src="https://github.com/robinroy03/fury-discord-bot/assets/115863770/234fee85-9eb4-4fd5-a334-9e6d11e552a3" width="640" height="390" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;ol class="arabic simple" start="4"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Dockerizing Discord Bot&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-16-week3-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “4” (ordinal 4)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;I earlier used the huggingface implementation (copied their implementation demo). It was bad. My mentors suggested to dockerize the bot so I did that.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Benchmarking. Now we have the data, but we need to properly benchmark to see whether the modifications I make every day are making the bot dumber or smarter.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Study different techniques to improve model answer accuracy such as &lt;a class="reference external" href="https://arxiv.org/abs/2212.10496"&gt;HyDE&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Study how to go forward with fine-tuning.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Improved references.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Collect more data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;No, everything went well this week. Exam preparation was a pain though😢.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://developers.googleblog.com/en/gemini-15-pro-and-15-flash-now-available"&gt;Gemini Blog&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-16-week3-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id1"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “hyde”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://arxiv.org/abs/2212.10496"&gt;HyDE&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin :)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-06-16-week3-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 3.</summary>
    <category term="google" label="google"/>
    <published>2024-06-16T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-06-16-week2-robin.html</id>
    <title>Week 2: The first iteration!</title>
    <updated>2024-06-16T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-2-the-first-iteration"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 2.&lt;/p&gt;
&lt;p&gt;My goal for week 2 was to connect everything and make a prototype. So now we have a bot working 24x7 to answer all your doubts :)&lt;/p&gt;
&lt;dl class="simple"&gt;
&lt;dt&gt;Apart from the things mentioned in my &lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-06-06-week-1-robin.html"&gt;week 1 blog&lt;/a&gt;, the things I did in week 2 are basically:&lt;/dt&gt;&lt;dd&gt;&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Chunking the files for embedding.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Upserting the chunks into the database.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Connecting everything together.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Making the discord bot async.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Merging a PR.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;section id="chunking-the-files-for-embedding"&gt;
&lt;h2&gt;1) &lt;strong&gt;Chunking the files for embedding&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In the context of building LLM-related applications, chunking is the process of breaking down large pieces of text into smaller segments. It’s an essential technique that helps optimize the relevance of the content we get back from a vector database once we use an embedding model to embed content. For our case with FURY, our data is entirely code. So one approach I tried was to take docstrings and the function/class signature.&lt;/p&gt;
&lt;p&gt;I used a naive parser during week 2, which used a combination of regex and common pattern matching to do this splitting. Later my mentors &lt;a class="reference external" href="https://github.com/m-agour"&gt;Mohamed&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/skoudoro/"&gt;Serge&lt;/a&gt; told me to use a better approach, using the python &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;inspect&lt;/span&gt;&lt;/code&gt; module.&lt;/p&gt;
&lt;p&gt;Another thing to consider was the chunk size. It is shown that smaller chunks outperform larger chunks. This can be intuitively thought of like this: An embedding model can compress a smaller text to 1024 vectors without much data loss compared to compressing a larger text to 1024 vectors.&lt;/p&gt;
&lt;p&gt;This also introduces another important issue, we need a way to test it based on our model. So we need benchmarking.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="upserting-chunks-into-the-database"&gt;
&lt;h2&gt;2) &lt;strong&gt;Upserting chunks into the database&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;I upserted all the chunks into the database, along with the vectors I gave metadata which was the function signature and docstrings. Later in week 3, we’ll modify this to show links instead of the big wall of text.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="connecting-everything-together"&gt;
&lt;h2&gt;3) &lt;strong&gt;Connecting everything together&lt;/strong&gt;&lt;/h2&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-16-week2-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id1"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “week 1 blog”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;I took the 4 key parts - Discord Bot, LLM API, Embeddings API and the Database API and connected them together. This was explained on the &lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-06-06-week-1-robin.html"&gt;week 1 blog&lt;/a&gt; itself.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="making-the-discord-bot-async"&gt;
&lt;h2&gt;4) &lt;strong&gt;Making the Discord Bot async&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;One of the biggest challenges I faced this week was to get everything running properly. LLM output takes a lot of time to generate (we’ll fix this amazingly well in week 3 BTW).
I made a big mistake, I used &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;requests&lt;/span&gt;&lt;/code&gt; library to do the REST API calls. It occurred to me later that it is synchronous and does blocking calls. This was the reason my Discord bot was dying randomly. I fixed it by migrating to &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;aiohttp&lt;/span&gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This also made me realize I can use async in a lot of other places. A lot of these tasks are I/O bound. If I make them async we might be able to take many more concurrent requests.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="merging-a-pr"&gt;
&lt;h2&gt;5) &lt;strong&gt;Merging a PR&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;I merged a &lt;a class="reference external" href="https://github.com/fury-gl/fury/pull/893"&gt;PR&lt;/a&gt; which modifies &lt;cite&gt;.gitignore&lt;/cite&gt;. I found this while generating the Sphinx docs.&lt;/p&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h3&gt;What is coming up next week?&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;A faster LLM inference.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Better pipeline for data collection.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Links for citation.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h3&gt;Did you get stuck anywhere?&lt;/h3&gt;
&lt;p&gt;Took me some time to realize I was using synchronous code inside async. Fixed it later.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-16-week2-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id2"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “week 1 blog”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://fury.gl/latest/posts/2024/2024-06-06-week-1-robin.html"&gt;Week 1 Blog&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-16-week2-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id3"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “pr”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/fury-gl/fury/pull/893"&gt;PR&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/skoudoro/"&gt;Serge Koudoro&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/m-agour"&gt;Mohamed Abouagour&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin :)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-06-16-week2-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 2.</summary>
    <category term="google" label="google"/>
    <published>2024-06-16T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-06-06-week-1-robin.html</id>
    <title>Week 1: It officially begins…</title>
    <updated>2024-06-06T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-1-it-officially-begins"&gt;

&lt;p&gt;Hi, I’m &lt;a class="reference external" href="https://github.com/robinroy03"&gt;Robin&lt;/a&gt; and this is my blog about week 1.&lt;/p&gt;
&lt;p&gt;My goal for week 1 was to start with &lt;a class="reference external" href="https://www.pinecone.io/learn/retrieval-augmented-generation/"&gt;Retrieval-Augmented Generation (RAG)&lt;/a&gt;, check different databases and host every endpoint. My week1 and week2 are very intertwined because I applied everything I did during week1 on week2. (I’m writing this blog midway through week2)&lt;/p&gt;
&lt;section id="why-phi-3-mini-4k-instruct"&gt;
&lt;h2&gt;why phi-3-mini-4k-instruct?&lt;/h2&gt;
&lt;p&gt;Before I detail everything I’ve done this week, I’ll explain why &lt;a class="reference external" href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"&gt;phi-3 mini 4k&lt;/a&gt; was chosen as the LLM, I forgot to mention this in the last blog. Phi-3 is a small 3.8B 4k context model, it means it can work with 4k tokens(similar to words) at a time. Due to its small size, it runs fast both locally and on Huggingface. Performance wise comparatively with other opensource models, it performs decently well. In the &lt;a class="reference external" href="https://chat.lmsys.org/?leaderboard"&gt;LMSYS LLM leaderboard&lt;/a&gt; phi-3 mini 4k comes with an ELO of 1066 (59th position). But it achieves this as a small model.
I also tried Llama3-8B, it performs better than phi-3 mini with an ELO of 1153 and rank 22. But it is considerably slower for inference. Due to this, I chose phi-3 mini for now.&lt;/p&gt;
&lt;section id="things-i-did-week-1-and-some-week2"&gt;
&lt;h3&gt;Things I did week-1 (and some week2)&lt;/h3&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Choosing the vector database&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I decided to choose &lt;a class="reference external" href="https://www.pinecone.io/"&gt;Pinecone&lt;/a&gt; as the vector DB because it had a very generous free tier. Other options on consideration were &lt;a class="reference external" href="https://github.com/pgvector/pgvector"&gt;pgvector&lt;/a&gt; and &lt;a class="reference external" href="https://www.trychroma.com/"&gt;chromadb&lt;/a&gt;, but they didn’t have a free tier.&lt;/p&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;PR Submissions and Review&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;I also merged a &lt;a class="reference external" href="https://github.com/fury-gl/fury/pull/891"&gt;PR&lt;/a&gt; on FURY which fixes a CI issue. I also spent time doing review of other PRs from my fellow GSoC mates.&lt;/p&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Deciding which embedding model to use&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “3” (ordinal 3)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;A good embedding model is necessary to generate embeddings which we then upsert into the DB. Ollama had embedding model support, but I found the catalogue very small and the models they provided were not powerful enough. Therefore I decided to try using HuggingFace Sentence Transformers.
Sentence Transformers have a very vibrant catalogue of models available of various sizes. I chose &lt;a class="reference external" href="https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5"&gt;gte-large-en-v1.5&lt;/a&gt; from Alibaba-NLP, an 8k context, 434 million parameter model. It only had a modest memory requirement of 1.62 GB.
Performance wise, it ranks 11th on the &lt;a class="reference external" href="https://huggingface.co/spaces/mteb/leaderboard"&gt;MTEB leaderboard&lt;/a&gt;. It is a very interesting model due to its size:performance ratio.&lt;/p&gt;
&lt;ol class="arabic simple" start="4"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hosting the embedding model&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “4” (ordinal 4)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;Hosting this sentence-transformer model was confusing. For some reason, the HF spaces were blocking the Python script from writing on &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;.cache&lt;/span&gt;&lt;/code&gt; folder. Docker container inside spaces runs with user id 1000 (non-root user), therefore I had to give it permission to download and store files.&lt;/p&gt;
&lt;p&gt;I’ve hosted 5 gunicorn workers to serve 5 parallel requests at a time. Since the model is small, this is possible.&lt;/p&gt;
&lt;ol class="arabic simple" start="5"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hosting the database endpoint&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “5” (ordinal 5)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;I wrapped the pinecone DB API into an endpoint so it’ll be easy to query and receive the results.
It is also configured to accept 5 concurrent requests although I could increase it a lot more.&lt;/p&gt;
&lt;p&gt;I upserted docstrings from &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;fury/actor.py&lt;/span&gt;&lt;/code&gt; into the vector DB for testing. So now, whenever you ask a question the model will use some &lt;code class="docutils literal notranslate"&gt;&lt;span class="pre"&gt;actor.py&lt;/span&gt;&lt;/code&gt; function to give you an answer. For now, it could be used like a semantic function search engine.&lt;/p&gt;
&lt;p&gt;I decided to abstract the DB endpoint to reduce the dependency on one provider. We can swap the providers as required and keep all other features running.&lt;/p&gt;
&lt;ol class="arabic simple" start="6"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hosting Discord Bot&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “6” (ordinal 6)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;So with this, all the endpoints are finally online. The bot has some issues, it is going offline midway for some reason. I’ll have to see why that happens.&lt;/p&gt;
&lt;p&gt;For some reason, Huggingface spaces decided to not start the bot script. Later a community admin from Huggingface told me to use their official bot implementation as a reference. This is why I had to use threading and gradio to get the bot running (migrating to docker can be done, but this is how they did it and I just took that for now).&lt;/p&gt;
&lt;p&gt;Huggingface spaces need a script to satisfy certain criteria to allow them to run, one of them is a non-blocking I/O on the main loop. So I had to move the discord bot to a new thread.&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="connecting-all-of-them-together"&gt;
&lt;h2&gt;Connecting all of them together!&lt;/h2&gt;
&lt;dl class="simple"&gt;
&lt;dt&gt;So now we have 4 hosted services, all hosted on HuggingFace spaces:&lt;/dt&gt;&lt;dd&gt;&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;Discord Bot&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;LLM API&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Embeddings API&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Database API&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Now we’ll have to connect them all to get an answer to the user query.&lt;/p&gt;
&lt;p&gt;This is the current architecture, there’s a lot of room for improvement here.&lt;/p&gt;
&lt;blockquote&gt;
&lt;div&gt;&lt;img src="https://github.com/fury-gl/fury-communication-assets/blob/main/gsoc_2024/7-6-2024-demo-architecture-gsoc-robin-week2.png?raw=true"&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;p&gt;The Language Model takes the context and the user query, combines them to form an answer and returns to the user through discord (for now). Maybe moving the core logic from discord bot to a separate node might be good, and connect discord/github/X to that node.
The database takes embeddings and do an Approximate Nearest Neighbor search (a variant of KNN) and returns top-k results (k=3 for now).&lt;/p&gt;
&lt;blockquote&gt;
&lt;div&gt;&lt;iframe src="https://github.com/robinroy03/fury-discord-bot/assets/115863770/48f1136d-18a5-45ee-aa22-0a3f6426d575" width="640" height="390" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h3&gt;What is coming up next week?&lt;/h3&gt;
&lt;p&gt;Answer quality improvements. Also, the discord bot dies randomly, have to fix that also.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h3&gt;Did you get stuck anywhere?&lt;/h3&gt;
&lt;p&gt;Was stuck in hosting models on Huggingface spaces, fixed it later.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/fury-bot-discord/tree/main"&gt;Discord Bot&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/fury-db-endpoint/tree/main"&gt;Database Repo&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/fury-embeddings-endpoint/tree/main"&gt;Embedding Repo&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/fury-bot/tree/main"&gt;LLM Repo&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id1"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “retrieval-augmented generation (rag)”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://www.pinecone.io/learn/retrieval-augmented-generation/"&gt;Retrieval-Augmented Generation (RAG)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id2"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “phi-3 mini 4k”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"&gt;phi-3 mini 4k&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id3"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “lmsys llm leaderboard”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://chat.lmsys.org/?leaderboard"&gt;LMSYS LLM leaderboard&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id4"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “pinecone”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://www.pinecone.io/"&gt;Pinecone&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id5"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “pgvector”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/pgvector/pgvector"&gt;pgvector&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id6"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “chromadb”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://www.trychroma.com/"&gt;chromadb&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id7"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “pr”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/fury-gl/fury/pull/891"&gt;PR&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id8"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “gte-large-en-v1.5”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5"&gt;gte-large-en-v1.5&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-06-06-week-1-robin.rst&lt;/span&gt;, line 2); &lt;em&gt;&lt;a href="#id9"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Duplicate explicit target name: “mteb leaderboard”.&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/mteb/leaderboard"&gt;MTEB leaderboard&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-06-06-week-1-robin.html"/>
    <summary>Hi, I’m Robin and this is my blog about week 1.</summary>
    <category term="google" label="google"/>
    <published>2024-06-06T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://fury.gl/posts/2024/2024-05-28-week-0-robin.html</id>
    <title>Week 0: Community Bonding!</title>
    <updated>2024-05-29T00:00:00+00:00</updated>
    <author>
      <name>Robin Roy</name>
    </author>
    <content type="html">&lt;section id="week-0-community-bonding"&gt;

&lt;p&gt;Hi, I’m Robin and I’m a 2nd year CS undergrad from Vellore Institute of Technology, Chennai. During GSoC ‘24 my work will be to build an LLM chatbot which will help the community by answering their questions.&lt;/p&gt;
&lt;p&gt;Scientific visualization is often complicated and hard for people to get used to - “Although 3D visualization technologies are advancing quickly, their sophistication and focus on non-scientific domains makes it hard for researchers to use
them. In other words, most of the existing 3D visualization and computing APIs are low-level
(close to the hardware) and made for professional specialist developers.” &lt;a class="reference internal" href="../../posts/2024/2024-05-28-week-0-robin.html#fury" id="id1"&gt;&lt;span&gt;[FURY]&lt;/span&gt;&lt;/a&gt;. FURY is our effort to bridge this gap with an easy-to-use API. With LLMs, the goal is to take this one step further and make it even simpler for people to get started. By reducing the barrier to entry, we can bring more people into this domain. Visualization should not be the most time-consuming thing for an engineer/researcher, it is supposed to just happen and help them accelerate faster.&lt;/p&gt;
&lt;section id="my-community-bonding-work"&gt;
&lt;h2&gt;My Community Bonding Work&lt;/h2&gt;
&lt;p&gt;The main goal for me was to try different hosting providers and LLMs, test everything and see how they perform. I had my final exams during this period so I lost around 2 weeks to that. But I did manage to catch up and finish the work.
We wanted to keep hosting cheap (preferably free). I’ll detail all the things I tried and the review for each of them.&lt;/p&gt;
&lt;section id="hosting-work-in-order"&gt;
&lt;h3&gt;Hosting work, in order:&lt;/h3&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ollama on Google Colab&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The way it works is by taking Ollama and running it inside google colab, then providing a reverse proxy using ngrok.
We later connect that reverse proxy to the local ollama instance.&lt;/p&gt;
&lt;p&gt;It works. But Google Colab can run only for a maximum of 12 hours and the runtimes will timeout if idle. Also, it was very hacky.&lt;/p&gt;
&lt;blockquote&gt;
&lt;div&gt;&lt;iframe width="640" height="390" src="https://drive.google.com/file/d/1qNLtXxAMlLQ8xO8jfV0keRtskvcsj-fC/preview" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;ol class="arabic simple" start="2"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ollama on Kaggle&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-05-28-week-0-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “2” (ordinal 2)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;Same as above, same issues. Talked with my mentor &lt;a class="reference external" href="https://github.com/m-agour"&gt;Mohamed&lt;/a&gt; and skipped implementation.&lt;/p&gt;
&lt;ol class="arabic simple" start="3"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;GGUF (GPT-Generated Unified Format) models with ctransformers on HuggingFace&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-05-28-week-0-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “3” (ordinal 3)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;The way it works is by taking a &lt;a class="reference external" href="https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/"&gt;gguf&lt;/a&gt; model and then inferencing using the ctransformers library from HuggingFace. An endpoint will be exposed using flask/fastapi.&lt;/p&gt;
&lt;p&gt;It had issues like not all models were working, and ctransformers did not support all models. And the models that do work were slow on my machine. Local testing was a nightmare and inference speed on HuggingFace was also very slow.&lt;/p&gt;
&lt;ol class="arabic simple" start="4"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;GGUF models with llama-cpp-python, hosted on HuggingFace&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-05-28-week-0-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “4” (ordinal 4)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;I used langchain wrapper over llama-cpp-python to inference GGUF models. This one was able to handle all GGUF models, and local testing was okayish. When I tried handling concurrent requests, it crashed and gave segmentation fault. I fixed segmentation fault later by increasing gunicorn workers (Gunicorn was the WSGI server I used).
It was still not that good and local testing was annoying me. I cannot iterate fast when it takes a full 2-3 minutes for the output to generate.&lt;/p&gt;
&lt;p&gt;This wrapper on a wrapper on a wrapper was also not fun (langchain wrapper of llama-cpp-python which itself is a wrapper of llama-cpp).&lt;/p&gt;
&lt;p&gt;I later removed langchain and reimplemented everything, but langchain wasn’t the reason for the slow performance so it wasn’t helpful.&lt;/p&gt;
&lt;ol class="arabic simple" start="5"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ollama on HuggingFace!&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;aside class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: INFO/1 (&lt;span class="docutils literal"&gt;/home/runner/work/fury/fury/docs/source/posts/2024/2024-05-28-week-0-robin.rst&lt;/span&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Enumerated list start value not ordinal-1: “5” (ordinal 5)&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;TLDR: This one worked!&lt;/p&gt;
&lt;blockquote&gt;
&lt;div&gt;&lt;iframe width="640" height="390" src="https://drive.google.com/file/d/17yxdw169uqLlw6WKfi--bWEUQArJk7i2/preview" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;p&gt;Ollama was perfect, it works like a charm on my machine and the ecosystem is also amazing (the people on their discord server are super kind). I knew I had to try ollama on HuggingFace.
I was unable to initially run ollama and provide an endpoint. My dockerfile builds were all failing. Later mentor &lt;a class="reference external" href="https://github.com/skoudoro/"&gt;Serge&lt;/a&gt; told me to use the official Ollama image (till then I was using the Ubuntu base image).&lt;/p&gt;
&lt;p&gt;I managed to get the dockerfile running locally, but still, the HuggingFace build was failing. Then I took help from HuggingFace community. They told me it was HuggingFace blocking some ports, so to try different ports. This is when I came across another ollama server repo, and it was using Ubuntu as the base image. I studied that code and modified my dockerfile. It was adding an env variable to repo settings that I missed. My current dockerfile is just 5 lines and it works well.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="fury-discord-bot"&gt;
&lt;h3&gt;FURY Discord Bot&lt;/h3&gt;
&lt;p&gt;I also made a barebones FURY Discord Bot which was connected to my local ollama instance. My dockerfile was stuck and I wanted to do something tangible, so I did this before the weekly meeting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;div&gt;&lt;iframe src="https://drive.google.com/file/d/17aosa4iyDl90mYfVGPrmILtQdXtS6IEy/preview" width="640" height="480" allow="autoplay"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/blockquote&gt;
&lt;/section&gt;
&lt;/section&gt;
&lt;section id="what-is-coming-up-next-week"&gt;
&lt;h2&gt;What is coming up next week?&lt;/h2&gt;
&lt;p&gt;Currently, I’m finding a vector DB &amp;amp; studying how to effectively use RAG here.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="did-you-get-stuck-anywhere"&gt;
&lt;h2&gt;Did you get stuck anywhere?&lt;/h2&gt;
&lt;p&gt;Yes, I had some issues with the dockerfile. It was resolved.&lt;/p&gt;
&lt;p&gt;LINKS:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://huggingface.co/spaces/robinroy03/fury-bot/tree/main"&gt;HuggingFace repo&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/robinroy03/fury-discord-bot"&gt;Discord Bot&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thank you for reading!&lt;/p&gt;
&lt;div role="list" class="citation-list"&gt;
&lt;div class="citation" id="fury" role="doc-biblioentry"&gt;
&lt;span class="label"&gt;&lt;span class="fn-bracket"&gt;[&lt;/span&gt;&lt;a role="doc-backlink" href="#id1"&gt;FURY&lt;/a&gt;&lt;span class="fn-bracket"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;p&gt;Eleftherios Garyfallidis, Serge Koudoro, Javier Guaje, Marc-Alexandre Côté, Soham Biswas, David Reagan, Nasim Anousheh, Filipi Silva, Geoffrey Fox, and Fury Contributors. “FURY: advanced scientific visualization.” Journal of Open Source Software 6, no. 64 (2021): 3384. &lt;a class="reference external" href="https://doi.org/10.21105/joss.03384"&gt;https://doi.org/10.21105/joss.03384&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;
&lt;/section&gt;
</content>
    <link href="https://fury.gl/posts/2024/2024-05-28-week-0-robin.html"/>
    <summary>Hi, I’m Robin and I’m a 2nd year CS undergrad from Vellore Institute of Technology, Chennai. During GSoC ‘24 my work will be to build an LLM chatbot which will help the community by answering their questions.</summary>
    <category term="google" label="google"/>
    <published>2024-05-29T00:00:00+00:00</published>
  </entry>
</feed>
